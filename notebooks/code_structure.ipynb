{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Video \n",
    "* load video - done\n",
    "* Split into frames - done\n",
    "* Save images with frame info - done, saved with frame number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fer import Video\n",
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import *\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install FER\\n!pip install moviepy\\n!pip install tensorflow-macos==2.8.0\\n!pip install tensorflow-metal==0.4.0\\n!pip install Keras==2.8.0\\n!pip install tensorflow-hub==0.12.0\\n!pip install --upgrade protobuf==3.20.1\\n!pip install opencv-python  '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip install FER\n",
    "!pip install moviepy\n",
    "!pip install tensorflow-macos==2.8.0\n",
    "!pip install tensorflow-metal==0.4.0\n",
    "!pip install Keras==2.8.0\n",
    "!pip install tensorflow-hub==0.12.0\n",
    "!pip install --upgrade protobuf==3.20.1\n",
    "!pip install opencv-python  '''    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames:  582\n",
      "Converting video..\n",
      "\n",
      "Done extracting frames.\n",
      "582 frames extracted\n"
     ]
    }
   ],
   "source": [
    "def video_to_frames(input_loc, output_loc):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "    # Find the number of frames\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print (\"Number of frames: \", video_length)\n",
    "    count = 0\n",
    "    print (\"Converting video..\\n\")\n",
    "    # Start converting the video\n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        # Write the results back to output location.\n",
    "        cv2.imwrite(output_loc + \"/%#05d.jpg\" % (count+1), frame)\n",
    "        count = count + 1\n",
    "        # If there are no more frames left\n",
    "        if (count > (video_length-1)):\n",
    "            # Release the feed\n",
    "            cap.release()\n",
    "            # Print stats\n",
    "            print (\"Done extracting frames.\\n%d frames extracted\" % count)\n",
    "            break\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    input_loc = \"07. Emotion Recognition using Live Video/Input/FER_Videos/Video_One.mp4\"\n",
    "    output_loc = '/Users/mahaabu-khousa/Desktop/ai-with-python-series/frames/'\n",
    "    video_to_frames(input_loc, output_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cap.read()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Image\t\n",
    "* Load\n",
    "* Maybe preformat\n",
    "\n",
    "* Face Recognition, extract coodinates\n",
    "* Cut faces out\n",
    "* Format\n",
    "* Save cropped face with frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, extract_face\n",
    "from PIL import Image, ImageDraw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    for img in frames:\\n           boxes, probs = mtcnn.detect(img, landmarks=False)\\n# Draw boxes and save faces\\nimg_draw = img.copy()\\ndraw = ImageDraw.Draw(img_draw)\\nfor i, (box, point) in enumerate(zip(boxes, points)):\\n    draw.rectangle(box.tolist(), width=5)\\n    for p in point:\\n    draw.rectangle((p - 10).tolist() + (p + 10).tolist(), width=10)\\n    extract_face(img, box, save_path='detected_face_{}.png'.format(i))\\nimg_draw.save('annotated_faces.png')\\n\\n\\n\\n\\n    if bgr:\\n                gray_image_array = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def face_detector(cascade_file: str = None,\n",
    "        mtcnn=True,\n",
    "        tfserving: bool = False,\n",
    "        scale_factor: float = 1.1,\n",
    "        min_face_size: int = 50,\n",
    "        min_neighbors: int = 5,\n",
    "        offsets: tuple = (10, 10)):\n",
    "    if cascade_file is None:\n",
    "            cascade_file = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"    \n",
    "    if mtcnn:\n",
    "            mtcnn = MTCNN(keep_all=True)\n",
    "    else:\n",
    "            face_detector_cascade = cv2.CascadeClassifier(cascade_file)\n",
    "    return mtcnn\n",
    "    \n",
    "'''\n",
    "    for img in frames:\n",
    "           boxes, probs = mtcnn.detect(img, landmarks=False)\n",
    "# Draw boxes and save faces\n",
    "img_draw = img.copy()\n",
    "draw = ImageDraw.Draw(img_draw)\n",
    "for i, (box, point) in enumerate(zip(boxes, points)):\n",
    "    draw.rectangle(box.tolist(), width=5)\n",
    "    for p in point:\n",
    "    draw.rectangle((p - 10).tolist() + (p + 10).tolist(), width=10)\n",
    "    extract_face(img, box, save_path='detected_face_{}.png'.format(i))\n",
    "img_draw.save('annotated_faces.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if bgr:\n",
    "                gray_image_array = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = face_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MTCNN(\n",
       "  (pnet): PNet(\n",
       "    (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu1): PReLU(num_parameters=10)\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu2): PReLU(num_parameters=16)\n",
       "    (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu3): PReLU(num_parameters=32)\n",
       "    (conv4_1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (softmax4_1): Softmax(dim=1)\n",
       "    (conv4_2): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (rnet): RNet(\n",
       "    (conv1): Conv2d(3, 28, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu1): PReLU(num_parameters=28)\n",
       "    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv2): Conv2d(28, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu2): PReLU(num_parameters=48)\n",
       "    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv3): Conv2d(48, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (prelu3): PReLU(num_parameters=64)\n",
       "    (dense4): Linear(in_features=576, out_features=128, bias=True)\n",
       "    (prelu4): PReLU(num_parameters=128)\n",
       "    (dense5_1): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (softmax5_1): Softmax(dim=1)\n",
       "    (dense5_2): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       "  (onet): ONet(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu1): PReLU(num_parameters=32)\n",
       "    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu2): PReLU(num_parameters=64)\n",
       "    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu3): PReLU(num_parameters=64)\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv4): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (prelu4): PReLU(num_parameters=128)\n",
       "    (dense5): Linear(in_features=1152, out_features=256, bias=True)\n",
       "    (prelu5): PReLU(num_parameters=256)\n",
       "    (dense6_1): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (softmax6_1): Softmax(dim=1)\n",
       "    (dense6_2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    (dense6_3): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread('/Users/mahaabu-khousa/Desktop/ai-with-python-series/frames/00562.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_faces(img):\n",
    "    boxes, probs = mtcnn.detect(img)  \n",
    "    faces = []\n",
    "    if type(boxes) == np.ndarray:\n",
    "        for face in boxes:\n",
    "            faces.append(\n",
    "                [\n",
    "                    int(face[0]),\n",
    "                    int(face[1]),\n",
    "                    int(face[2]) - int(face[0]),\n",
    "                    int(face[3]) - int(face[1]),\n",
    "                ]\n",
    "            )\n",
    "    return boxes, probs, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, probs, face_rectangles = find_faces(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999243], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[796, 272, 273, 341]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_rectangles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes_to_square(face_rectangles):\n",
    "    offsets = (10,10)\n",
    "    for face_coordinates in face_rectangles:\n",
    "        x, y, w, h = face_coordinates\n",
    "        if h > w:\n",
    "            diff = h - w\n",
    "            x -= diff // 2\n",
    "            w += diff\n",
    "        elif w > h:\n",
    "            diff = w - h\n",
    "            y -= diff // 2\n",
    "            h += diff\n",
    "        if w != h:\n",
    "            print(f\"{w} is not {h}\")\n",
    "\n",
    "        x_off, y_off = offsets\n",
    "        x1 = x - x_off\n",
    "        x2 = x + w + x_off\n",
    "        y1 = y - y_off\n",
    "        y2 = y + h + y_off\n",
    "        \n",
    "    return x1, x2, y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(752, 1113, 262, 623)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2, y1, y2 = boxes_to_square(face_rectangles)\n",
    "\n",
    "x1, x2, y1, y2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
