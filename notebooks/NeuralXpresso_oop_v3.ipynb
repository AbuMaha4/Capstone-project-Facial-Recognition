{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from facenet_pytorch import MTCNN\n",
    "import face_recognition\n",
    "\n",
    "from pytube import YouTube\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, yt_link, batch_size=50, skip_frames=1, video_output = False):\n",
    "        self.yt_link = yt_link\n",
    "        self.batch_size = batch_size\n",
    "        self.skip_frames = skip_frames\n",
    "        self.video_output = video_output\n",
    "        self.frames_without_faces = 0\n",
    "        self.video_emotions_by_frame = []\n",
    "        self.video_info_by_frame = []\n",
    "\n",
    "        self.load_video_from_youtube()\n",
    "\n",
    "\n",
    "    def load_video_from_youtube(self):\n",
    "\n",
    "        # Download the YouTube video and get the highest resolution stream\n",
    "        yt_video = YouTube(self.yt_link)\n",
    "        stream = yt_video.streams.get_highest_resolution()\n",
    "\n",
    "        # Open the video stream using OpenCV\n",
    "        self.video = cv2.VideoCapture(stream.url)\n",
    "\n",
    "        # Get available and used resolution - Debugging\n",
    "        #self.available_resolutions = [streams.resolution for streams in yt_video.streams.filter(type=\"video\", progressive=True)]\n",
    "        #self.used_resolution = stream.resolution\n",
    "\n",
    "        # Get the number of frames in the video\n",
    "        self.frame_count = int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Get the frame rate of the video\n",
    "        self.fps = int(self.video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "        # Get the height and width of the video frames\n",
    "        self.height = int(self.video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.width = int(self.video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "        # Calculate the total number of frames to process after skipping frames\n",
    "        self.total_frames = len(range(0, self.frame_count, self.skip_frames))\n",
    "\n",
    "        # Calculate the number of batches required to process all the frames\n",
    "        self.num_batches = int(np.ceil(self.total_frames / self.batch_size))\n",
    "\n",
    "    def get_batches(self):\n",
    "        # Initialize an empty numpy array to hold the frames\n",
    "        frames = np.empty((self.batch_size, self.height, self.width, 3), np.dtype('uint8'))\n",
    "\n",
    "        self.frames_read = 0\n",
    "\n",
    "        # Read the frames in batches and fill up the numpy array\n",
    "        for batch_start in range(0, self.frame_count, self.batch_size * self.skip_frames):\n",
    "            batch_end = min(batch_start + (self.batch_size * self.skip_frames), self.frame_count)\n",
    "            batch_index = 0\n",
    "\n",
    "            for i in range(batch_start, batch_end):\n",
    "                ret, frame = self.video.read()\n",
    "                self.frames_read +=1\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                if i % self.skip_frames == 0:\n",
    "                    frames[batch_index] = frame\n",
    "                    batch_index += 1\n",
    "\n",
    "            # Resize the numpy array to fit the actual number of frames in the batch\n",
    "            if batch_index < self.batch_size:\n",
    "                frames = frames[:batch_index]\n",
    "\n",
    "            # Yield the current batch of frames\n",
    "            yield frames\n",
    "\n",
    "        # Release the video stream\n",
    "        self.video.release()\n",
    "    \n",
    "    def get_overview_df(self, plottable = False):\n",
    "\n",
    "        frame_info_cols = ['frame', 'person_ID']\n",
    "\n",
    "        df_emotions = pd.DataFrame(self.video_emotions_by_frame, columns=EmotionDetector.get_emotion_categories())\n",
    "        df_frame_info = pd.DataFrame(self.video_info_by_frame, columns=frame_info_cols)\n",
    "        df = pd.concat([df_emotions, df_frame_info], axis=1)\n",
    "        \n",
    "        if plottable:\n",
    "            df = pd.melt(df, id_vars=frame_info_cols, value_vars=EmotionDetector.get_emotion_categories(), var_name='emotion', value_name='probability')\n",
    "        return df\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self, frame_width, frame_height, detection_type=\"face_recognition\", offset = 1.2):\n",
    "        self.frame_width = frame_width\n",
    "        self.frame_height = frame_height\n",
    "        self.detection_type = detection_type\n",
    "        self.offset = offset\n",
    "\n",
    "        if self.detection_type == \"MTCNN\":\n",
    "            # Initialize the MTCNN face detector\n",
    "            self.face_detector = MTCNN(keep_all=True, post_process=False, margin=20)\n",
    "        elif self.detection_type == \"haarcascade\":\n",
    "            # Load the Haar Cascade face detector\n",
    "            self.face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        elif self.detection_type == \"face_recognition\":\n",
    "            # Package face_detector doesn't require object initialization\n",
    "            self.face_detector = None\n",
    "        else: \n",
    "            raise ValueError(\"Choose one of the implemented models, ya cunt!\")\n",
    "\n",
    "\n",
    "    def detect_faces(self, frame):\n",
    "        if self.detection_type == 'haarcascade':\n",
    "            frame_preprocessed = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            boxes = self.face_detector.detectMultiScale(frame_preprocessed, scaleFactor=1.3, minNeighbors=3)\n",
    "            \n",
    "        elif self.detection_type == 'MTCNN':\n",
    "            # No preprocessing needed\n",
    "            boxes, _ = self.face_detector.detect(frame)\n",
    "            \n",
    "        elif self.detection_type == 'face_recognition':\n",
    "            frame_preprocessed = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Package face_detector directly return boxes without object initialization\n",
    "            boxes = face_recognition.face_locations(frame_preprocessed, number_of_times_to_upsample=1)\n",
    "        \n",
    "        if (boxes is None) or (len(boxes) == 0):\n",
    "            return [] \n",
    "\n",
    "        # Convert boxes to normalized format\n",
    "        norm_boxes = self.get_norm_boxes(boxes)\n",
    "\n",
    "        valid_boxes = []\n",
    "        for box in norm_boxes: \n",
    "            box_rearranged = np.array([box[0],box[3], box[1], box[2]])\n",
    "            #box_rearranged = [box[0],box[3], box[1], box[2]]     <--- Look again inside maybe\n",
    "            face_landmark = face_recognition.face_landmarks(frame, [box_rearranged])[0]\n",
    "            if FaceDetector.valid_landmarks(face_landmark):\n",
    "                # check for squared\n",
    "                box = self.augment_box(box, self.frame_width, self.frame_height, self.offset)\n",
    "                valid_boxes.append(box) \n",
    "\n",
    "        return valid_boxes\n",
    "\n",
    "    def get_norm_boxes(self, boxes):\n",
    "        \"\"\"\n",
    "        Normalize the bounding box coordinates from MTCNN to numpy indexing format.\n",
    "        Output format: np.array(y_min, y_max, x_min, x_max)\n",
    "        \"\"\"\n",
    "        normalized_box = []\n",
    "\n",
    "        if self.detection_type == 'haarcascade':\n",
    "            for box in boxes:\n",
    "                x, y, w, h = box\n",
    "                normalized_box.append([y, y+h, x, x+w])\n",
    "            return np.array(normalized_box)    \n",
    "\n",
    "        elif self.detection_type == \"MTCNN\":\n",
    "            for box in boxes:\n",
    "                x_min, y_min, x_max, y_max = box.astype(int)\n",
    "                normalized_box.append([y_min, y_max, x_min, x_max])\n",
    "            return np.array(normalized_box)\n",
    "\n",
    "        elif self.detection_type =='face_recognition':\n",
    "            for box in boxes:\n",
    "                y_min, x_max, y_max, x_min = box\n",
    "                normalized_box.append([y_min, y_max, x_min, x_max])\n",
    "            return np.array(normalized_box)    \n",
    "    \n",
    "    @staticmethod\n",
    "    def valid_landmarks(face_landmark):\n",
    "        left_eye = face_landmark['left_eye']\n",
    "        right_eye = face_landmark['right_eye']\n",
    "        mouth = face_landmark['top_lip'] + face_landmark['bottom_lip']\n",
    "\n",
    "        left_eye_center = np.mean(left_eye, axis=0)\n",
    "        right_eye_center = np.mean(right_eye, axis=0)\n",
    "        mouth_center = np.mean(mouth, axis=0)\n",
    "\n",
    "        eye_distance = np.linalg.norm(right_eye_center - left_eye_center)\n",
    "        mouth_to_eye_distance = np.linalg.norm(mouth_center - left_eye_center)\n",
    "\n",
    "        return mouth_to_eye_distance > eye_distance * 0.8\n",
    "\n",
    "    @staticmethod\n",
    "    def augment_box(box, frame_width, frame_height, offset=1.2):\n",
    "        y_min, y_max, x_min, x_max = box\n",
    "        box_height = y_max - y_min\n",
    "        box_width = x_max - x_min\n",
    "\n",
    "        # Check if result will be too large to fit into frame, return original box if so\n",
    "        max_side = max(box_height, box_width)\n",
    "        if max_side * offset > min(frame_width, frame_height):\n",
    "            return box\n",
    "\n",
    "        # Calculate middle point of rectangle, redefine corner points from there by multiplying offset\n",
    "        center_x = (x_min + x_max) // 2\n",
    "        center_y = (y_min + y_max) // 2\n",
    "        \n",
    "        y_min = center_y - max_side * offset / 2\n",
    "        y_max = center_y + max_side * offset / 2\n",
    "        x_min = center_x - max_side * offset / 2\n",
    "        x_max = center_x + max_side * offset / 2\n",
    "\n",
    "        # Move box back into frame if scaled box lays outside some side\n",
    "        # As chack for too large box already was done in step 1, we can use elif\n",
    "        if x_min < 0 or y_min < 0 or x_max > frame_width or y_max > frame_height:\n",
    "            x_offset, y_offset = 0, 0\n",
    "            if x_min < 0:\n",
    "                x_offset = -x_min\n",
    "            elif x_max > frame_width:\n",
    "                x_offset = frame_width - x_max\n",
    "            if y_min < 0:\n",
    "                y_offset = -y_min\n",
    "            elif y_max > frame_height:\n",
    "                y_offset = frame_height - y_max\n",
    "            y_min += y_offset\n",
    "            y_max += y_offset\n",
    "            x_min += x_offset\n",
    "            x_max += x_offset\n",
    "\n",
    "        return [int(y_min), int(y_max), int(x_min), int(x_max)]            \n",
    "\n",
    "class EmotionDetector:\n",
    "    def __init__(self):\n",
    "        self.emotion_detector = load_model(\"../models/emotion_model.hdf5\", compile=False)\n",
    "        self.emotion_categories = self.get_emotion_categories()\n",
    "    \n",
    "    def predict(self, frame, box):\n",
    "\n",
    "        gray_cropped_face = EmotionDetector.crop_face(box, frame)\n",
    "\n",
    "        prob = self.emotion_detector.predict(gray_cropped_face)[0]  # check for underscore\n",
    "        return prob\n",
    "    \n",
    "    @staticmethod\n",
    "    def crop_face(box, frame):\n",
    "        face = frame[box[0]:box[1], box[2]:box[3]]\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        face = cv2.resize(face, (64, 64))\n",
    "        face = face.astype('float32')/ 255\n",
    "        face = np.expand_dims(face, axis=-1)\n",
    "        face = np.expand_dims(face, axis=0)\n",
    "\n",
    "        return face\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_emotion_categories():\n",
    "        return ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    \n",
    "class PersonIdentifier:\n",
    "    def __init__(self, threshold = 0.6):\n",
    "        self.threshold = threshold\n",
    "        self.individual_id_counter = 0\n",
    "        self.known_face_encodings = {}\n",
    "\n",
    "    def assignID(self, frame, box):\n",
    "        box_rearranged = np.array([box[0],box[3], box[1], box[2]])\n",
    "        #box_rearranged = [box[0],box[3], box[1], box[2]]     <--- Look again inside maybe\n",
    "        face_encoding = face_recognition.face_encodings(frame, [box_rearranged])[0]\n",
    "        min_distance = float(\"inf\")\n",
    "        matched_individual_id = None\n",
    "\n",
    "        # Potential for: median face calculation & Passing the list\n",
    "\n",
    "        for individual_id, individual_reference_face_encoding in self.known_face_encodings.items():\n",
    "            distance = face_recognition.face_distance([individual_reference_face_encoding], face_encoding)[0]\n",
    "            if distance < self.threshold: # Adjust this threshold value based on your requirements\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    matched_individual_id = individual_id\n",
    "        \n",
    "        if matched_individual_id is None:\n",
    "            self.individual_id_counter += 1\n",
    "            matched_individual_id = self.individual_id_counter\n",
    "            self.known_face_encodings[self.individual_id_counter] = face_encoding\n",
    "\n",
    "        return matched_individual_id      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:26:13.297770: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "#yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'                   # 1 Actor\n",
    "#yt_link = 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "#yt_link = 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "yt_link = 'https://www.youtube.com/watch?v=UECCHwh7bZE'\n",
    "\n",
    "my_test = VideoProcessor(yt_link, skip_frames=50, batch_size=10000, video_output = True)\n",
    "\n",
    "my_face_detector = FaceDetector(detection_type='face_recognition', \n",
    "                                frame_width = my_test.width, \n",
    "                                frame_height = my_test.height,\n",
    "                                offset = 1)\n",
    "my_emotion_detector = EmotionDetector()\n",
    "my_person_identifier = PersonIdentifier()\n",
    "\n",
    "frame_nr = 0\n",
    "\n",
    "if my_test.video_output == False:\n",
    "    for idx, batch in enumerate(my_test.get_batches()):\n",
    "        for frame in batch:\n",
    "            face_boxes = my_face_detector.detect_faces(frame)\n",
    "            \n",
    "            if len(face_boxes) == 0:\n",
    "                my_test.frames_without_faces += 1\n",
    "                continue\n",
    "            \n",
    "            for box in face_boxes: \n",
    "                my_test.video_emotions_by_frame.append(my_emotion_detector.predict(frame, box))\n",
    "                my_test.video_info_by_frame.append((frame_nr, my_person_identifier.assignID(frame, box)))\n",
    "            frame_nr +=1\n",
    "\n",
    "else:\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')    \n",
    "    writer =  cv2.VideoWriter('Output_video_new_Off1.mp4', fourcc, 10, (my_test.width, my_test.height)) \n",
    "    \n",
    "    for idx, batch in enumerate(my_test.get_batches()):\n",
    "        for frame in batch:\n",
    "            face_boxes = my_face_detector.detect_faces(frame)\n",
    "            \n",
    "            if len(face_boxes) == 0:\n",
    "                my_test.frames_without_faces += 1\n",
    "                continue\n",
    "            \n",
    "            for box in face_boxes: \n",
    "                prob = my_emotion_detector.predict(frame, box)\n",
    "                character_id = my_person_identifier.assignID(frame, box)\n",
    "\n",
    "                my_test.video_emotions_by_frame.append(prob)\n",
    "                my_test.video_info_by_frame.append((frame_nr, character_id))\n",
    "            \n",
    "                max_emotion, max_prob = np.argmax(prob), np.max(prob)\n",
    "                emotion_text = EmotionDetector.get_emotion_categories()[max_emotion]\n",
    "        \n",
    "                y_min, y_max, x_min, x_max = box\n",
    "        \n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Prob: {max_prob:.1%}\", (x_min, y_max + 60), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 1)\n",
    "                cv2.putText(frame, f\"{emotion_text}\", (x_min, y_max + 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 1)\n",
    "                cv2.putText(frame, f\"ID: {character_id}\", (x_min, y_min -20), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 1)\n",
    "            \n",
    "            \n",
    "            writer.write(frame)\n",
    "            frame_nr +=1\n",
    "    \n",
    "    writer.release() \n",
    "\n",
    "my_df = my_test.get_overview_df(plottable=False)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yt_link': 'https://www.youtube.com/watch?v=UECCHwh7bZE',\n",
       " 'batch_size': 10000,\n",
       " 'skip_frames': 25,\n",
       " 'video_output': True,\n",
       " 'frames_without_faces': 0,\n",
       " 'video_emotions_by_frame': [],\n",
       " 'video_info_by_frame': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Angry</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Happy</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>frame</th>\n",
       "      <th>person_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020894</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.068198</td>\n",
       "      <td>0.145649</td>\n",
       "      <td>0.072939</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.691310</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062610</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.111277</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.205002</td>\n",
       "      <td>0.027280</td>\n",
       "      <td>0.448589</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026876</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.021906</td>\n",
       "      <td>0.752188</td>\n",
       "      <td>0.026912</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.166458</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025669</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.260382</td>\n",
       "      <td>0.071585</td>\n",
       "      <td>0.135191</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.506314</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041262</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.147834</td>\n",
       "      <td>0.048683</td>\n",
       "      <td>0.548283</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>0.205529</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.078556</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>0.054074</td>\n",
       "      <td>0.138481</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>0.196434</td>\n",
       "      <td>89</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.053365</td>\n",
       "      <td>0.087438</td>\n",
       "      <td>0.580437</td>\n",
       "      <td>0.042181</td>\n",
       "      <td>0.040655</td>\n",
       "      <td>0.144925</td>\n",
       "      <td>89</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.504933</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>0.028646</td>\n",
       "      <td>0.013052</td>\n",
       "      <td>0.014140</td>\n",
       "      <td>0.372410</td>\n",
       "      <td>0.061250</td>\n",
       "      <td>89</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.033859</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.537065</td>\n",
       "      <td>0.025003</td>\n",
       "      <td>0.105364</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.294901</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.041639</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.277665</td>\n",
       "      <td>0.012979</td>\n",
       "      <td>0.496830</td>\n",
       "      <td>0.072769</td>\n",
       "      <td>0.097321</td>\n",
       "      <td>91</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Angry   Disgust      Fear     Happy       Sad  Surprise   Neutral  \\\n",
       "0    0.020894  0.000024  0.068198  0.145649  0.072939  0.000987  0.691310   \n",
       "1    0.062610  0.000774  0.111277  0.144467  0.205002  0.027280  0.448589   \n",
       "2    0.026876  0.000883  0.021906  0.752188  0.026912  0.004777  0.166458   \n",
       "3    0.025669  0.000347  0.260382  0.071585  0.135191  0.000511  0.506314   \n",
       "4    0.041262  0.002269  0.147834  0.048683  0.548283  0.006140  0.205529   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "248  0.078556  0.003998  0.523000  0.054074  0.138481  0.005457  0.196434   \n",
       "249  0.051000  0.053365  0.087438  0.580437  0.042181  0.040655  0.144925   \n",
       "250  0.504933  0.005568  0.028646  0.013052  0.014140  0.372410  0.061250   \n",
       "251  0.033859  0.000117  0.537065  0.025003  0.105364  0.003690  0.294901   \n",
       "252  0.041639  0.000797  0.277665  0.012979  0.496830  0.072769  0.097321   \n",
       "\n",
       "     frame  person_ID  \n",
       "0        0          1  \n",
       "1        0          2  \n",
       "2        1          3  \n",
       "3        1          1  \n",
       "4        1          2  \n",
       "..     ...        ...  \n",
       "248     89         46  \n",
       "249     89          3  \n",
       "250     89          7  \n",
       "251     90          6  \n",
       "252     91          6  \n",
       "\n",
       "[253 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>person_ID</th>\n",
       "      <th>emotion</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Angry</td>\n",
       "      <td>0.020894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "      <td>0.062610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Angry</td>\n",
       "      <td>0.026876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Angry</td>\n",
       "      <td>0.025669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "      <td>0.041262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>89</td>\n",
       "      <td>46</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.196434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>89</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.144925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>89</td>\n",
       "      <td>7</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.061250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.294901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>91</td>\n",
       "      <td>6</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.097321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1771 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      frame  person_ID  emotion  probability\n",
       "0         0          1    Angry     0.020894\n",
       "1         0          2    Angry     0.062610\n",
       "2         1          3    Angry     0.026876\n",
       "3         1          1    Angry     0.025669\n",
       "4         1          2    Angry     0.041262\n",
       "...     ...        ...      ...          ...\n",
       "1766     89         46  Neutral     0.196434\n",
       "1767     89          3  Neutral     0.144925\n",
       "1768     89          7  Neutral     0.061250\n",
       "1769     90          6  Neutral     0.294901\n",
       "1770     91          6  Neutral     0.097321\n",
       "\n",
       "[1771 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test.get_overview_df(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting FER\n",
      "  Using cached fer-22.5.0-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: matplotlib in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from FER) (3.7.1)\n",
      "Requirement already satisfied: pandas in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from FER) (1.5.3)\n",
      "Requirement already satisfied: requests in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from FER) (2.28.2)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting opencv-contrib-python\n",
      "  Using cached opencv_contrib_python-4.7.0.72-cp37-abi3-macosx_11_0_arm64.whl (41.0 MB)\n",
      "Requirement already satisfied: keras>=2.0.0 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from FER) (2.11.0)\n",
      "Requirement already satisfied: facenet-pytorch in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from FER) (2.5.2)\n",
      "Requirement already satisfied: torchvision in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from facenet-pytorch->FER) (0.15.1)\n",
      "Requirement already satisfied: pillow in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from facenet-pytorch->FER) (9.4.0)\n",
      "Requirement already satisfied: numpy in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from facenet-pytorch->FER) (1.24.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from matplotlib->FER) (1.4.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from matplotlib->FER) (5.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from matplotlib->FER) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from matplotlib->FER) (4.39.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from matplotlib->FER) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from matplotlib->FER) (23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from matplotlib->FER) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from matplotlib->FER) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from pandas->FER) (2022.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from requests->FER) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from requests->FER) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from requests->FER) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from requests->FER) (1.26.15)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->FER) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->FER) (1.16.0)\n",
      "Requirement already satisfied: torch==2.0.0 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from torchvision->facenet-pytorch->FER) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from torch==2.0.0->torchvision->facenet-pytorch->FER) (4.5.0)\n",
      "Requirement already satisfied: filelock in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from torch==2.0.0->torchvision->facenet-pytorch->FER) (3.10.0)\n",
      "Requirement already satisfied: sympy in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from torch==2.0.0->torchvision->facenet-pytorch->FER) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from torch==2.0.0->torchvision->facenet-pytorch->FER) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from torch==2.0.0->torchvision->facenet-pytorch->FER) (3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from jinja2->torch==2.0.0->torchvision->facenet-pytorch->FER) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages (from sympy->torch==2.0.0->torchvision->facenet-pytorch->FER) (1.3.0)\n",
      "Installing collected packages: tqdm, opencv-contrib-python, FER\n",
      "Successfully installed FER-22.5.0 opencv-contrib-python-4.7.0.72 tqdm-4.65.0\n"
     ]
    }
   ],
   "source": [
    "!pip install FER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512//2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
