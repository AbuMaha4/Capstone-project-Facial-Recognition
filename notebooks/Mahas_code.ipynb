{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 20:43:12.084446: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-16 20:43:12.084829: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-03-16 20:43:12.802710: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-16 20:43:12.920662: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pytube import YouTube\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "\n",
    "def download_video(yt_link):\n",
    "    yt = YouTube(yt_link)\n",
    "    stream = yt.streams.first()\n",
    "    stream.download()\n",
    "    video = cv2.VideoCapture(stream.default_filename)\n",
    "    return video\n",
    "\n",
    "\n",
    "def init_face_emotion_models():\n",
    "    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    emotion_classifier = load_model(\"../models/emotion_model.hdf5\", compile=False)\n",
    "    return face_detector, emotion_classifier\n",
    "\n",
    "def preprocess_face(face, input_face_size):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_RGB2GRAY)  # Convert the face to grayscale\n",
    "    face = cv2.resize(face, (input_face_size[1], input_face_size[0]))  # Swap width and height\n",
    "    face = face.astype('float32') / 255.0\n",
    "    face = np.expand_dims(face, axis=-1)  # Add an additional dimension for grayscale channel\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    return face\n",
    "\n",
    "\n",
    "\n",
    "def predict_emotion(face, emotion_classifier):\n",
    "    prob = emotion_classifier.predict(face)[0]\n",
    "    return np.argmax(prob), np.max(prob)\n",
    "\n",
    "\n",
    "def extract_faces_and_emotions(video, face_detector, emotion_classifier):\n",
    "    frames_without_faces_counter = 0\n",
    "    input_face_size = emotion_classifier.input_shape[1:3]\n",
    "    frame_rate = 50\n",
    "    skip_frames = int(video.get(cv2.CAP_PROP_FPS) / frame_rate)\n",
    "\n",
    "    list_of_faces = []\n",
    "    list_of_emotions = []\n",
    "    face_embeddings = {}\n",
    "    frames_without_faces_counter = 0\n",
    "    individual_id_counter = 0\n",
    "    frame_data = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "        face_locations = face_recognition.face_locations(rgb_frame)\n",
    "\n",
    "        # If there are no faces in the frame, skip it\n",
    "        if not face_locations:\n",
    "            continue\n",
    "        \n",
    "        for i, face_location in enumerate(face_locations):\n",
    "            top, right, bottom, left = face_location\n",
    "            face_landmark = face_recognition.face_landmarks(rgb_frame, [face_location])[0]\n",
    "            x, y, w, h = left, top, right - left, bottom - top\n",
    "            face = rgb_frame[top:bottom, left:right]\n",
    "            face = preprocess_face(face, input_face_size)\n",
    "            emotion, prob = predict_emotion(face, emotion_classifier)\n",
    "\n",
    "\n",
    "        for i, face_landmark in enumerate(face_landmark):\n",
    "            if not face_landmark:\n",
    "                continue\n",
    "\n",
    "            if i < len(face_locations):\n",
    "                face_encoding = face_recognition.face_encodings(rgb_frame, [face_locations[i]])[0]\n",
    "            else:\n",
    "                continue            \n",
    "            current_individual_ids = []\n",
    "\n",
    "            # Compare the current face encoding with the existing face embeddings\n",
    "            for individual_id, individual_face_encoding in face_embeddings.items():\n",
    "                if face_recognition.compare_faces([individual_face_encoding], face_encoding)[0]:\n",
    "                    current_individual_ids.append(individual_id)\n",
    "                    break\n",
    "            else:\n",
    "                individual_id_counter += 1\n",
    "                current_individual_ids.append(individual_id_counter)\n",
    "                face_embeddings[individual_id_counter] = face_encoding\n",
    "\n",
    "            # Preprocess face and predict emotion\n",
    "            face = rgb_frame[face_locations[i][0]:face_locations[i][2], face_locations[i][3]:face_locations[i][1]]\n",
    "            face = preprocess_face(face, input_face_size)\n",
    "            emotion, prob = predict_emotion(face, emotion_classifier)\n",
    "\n",
    "            # Append data to lists and dictionaries\n",
    "            frame_data.append({\n",
    "                'frame': video.get(cv2.CAP_PROP_POS_FRAMES),\n",
    "                'num_faces': len(faces),\n",
    "                'individual_id': current_individual_ids[-1],\n",
    "                'emotion': emotion,\n",
    "                'prob': prob\n",
    "            })\n",
    "            list_of_faces.append(face)\n",
    "            list_of_emotions.append(emotion_classifier.predict(face[np.newaxis, :, :, :].reshape(-1, 64, 64, 1))[0])\n",
    "\n",
    "            emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "            emotion_text = emotion_labels[emotion]\n",
    "\n",
    "            font_scale = 0.4  # Adjust the text size here\n",
    "\n",
    "            # Draw rectangle and text on the frame\n",
    "            x, y, w, h = left, top, right - left, bottom - top\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Prob: {prob:.1%}\", (x, y + h + 40), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 255), 1)\n",
    "            cv2.putText(frame, f\"{emotion_text}\", (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 255), 1)\n",
    "            cv2.putText(frame, f\"ID: {current_individual_ids[i]}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 0), 1)\n",
    "\n",
    "        scale_factor = 3\n",
    "        height, width = frame.shape[:2]\n",
    "        new_height = int(height * scale_factor)\n",
    "        new_width = int(width * scale_factor)\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Video', resized_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        else:\n",
    "            for i in range(skip_frames):\n",
    "                ret, _ = video.read()\n",
    "            frames_without_faces_counter = 0\n",
    "\n",
    "    return list_of_faces, list_of_emotions, face_embeddings, frame_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    yt_link = \"https://www.youtube.com/watch?v=pnFj9w3FGc8&ab_channel=PexBell-FreeStockVideoFootage\"\n",
    "    video = download_video(yt_link)\n",
    "    face_detector, emotion_classifier = init_face_emotion_models()\n",
    "\n",
    "    list_of_faces, list_of_emotions, face_embeddings, frame_data = extract_faces_and_emotions(video, face_detector, emotion_classifier)\n",
    "\n",
    "    print(f'{len(list_of_faces)} faces found in {len(list_of_emotions)} frames with at least one face.')\n",
    "    global frames_without_faces_counter\n",
    "    print(f'{frames_without_faces_counter} frames had no face detected ({round(100*frames_without_faces_counter/len(list_of_emotions),2)}%).')\n",
    "\n",
    "    # Save data to files\n",
    "    emotions = np.vstack(list_of_emotions)\n",
    "    np.savetxt('emotions.csv', emotions, delimiter=',')\n",
    "    emotions_df = pd.DataFrame(emotions, columns=['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'])\n",
    "    emotions_df['Frame'] = np.arange(len(emotions_df))\n",
    "\n",
    "    with open('face_embeddings.pickle', 'wb') as f:\n",
    "        pickle.dump(face_embeddings, f)\n",
    "\n",
    "    # Display the emotion detection results\n",
    "    individual_ids = {}\n",
    "    for data in frame_data:\n",
    "        individual_id = data['individual_id']\n",
    "        emotion = data['emotion']\n",
    "        prob = data['prob']\n",
    "        frame_num = data['frame']\n",
    "        if individual_id in individual_ids:\n",
    "            face, emotion_history = individual_ids[individual_id]\n",
    "            emotion_history[emotion] += 1\n",
    "        else:\n",
    "            if int(frame_num) < len(list_of_faces):\n",
    "                face = list_of_faces[int(frame_num)][0]\n",
    "                emotion_history = [0, 0, 0, 0, 0, 0, 0]\n",
    "                emotion_history[emotion] += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "    EMOTIONS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "    for individual_id, (face, emotion_history) in individual_ids.items():\n",
    "        x, y, w, h = face_locations[0]\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"ID: {individual_id}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        emotion_label = EMOTIONS[np.argmax(emotion_history)]\n",
    "        emotion_prob = emotion_history[np.argmax(emotion_history)] / sum(emotion_history)\n",
    "        cv2.putText(frame, f\"Emotion: {emotion_label} ({emotion_prob*100:.1f}%)\", (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
