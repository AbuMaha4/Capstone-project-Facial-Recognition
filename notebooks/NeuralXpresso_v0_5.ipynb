{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os           \n",
    "\n",
    "# Check if string is appropriate youtube link\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from facenet_pytorch import MTCNN\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import load_model\n",
    "from pytube import YouTube\n",
    "\n",
    "import face_recognition\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# DEBUG-Mode stops operations when max_emotions were detected and also prints a short summary\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    import time\n",
    "    debug_params = {\n",
    "        'max_emotions' : 500\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(kind):\n",
    "    if kind == \"youtube\":\n",
    "        #return \"https://www.youtube.com/watch?v=vtT78TfDfXU\"                   # Random video\n",
    "        return 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "        #return 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "    if kind == \"local\":\n",
    "        # adjust individually\n",
    "        return '/Users/steve/Neue_Fische/face_demo/vids/Video_One_output.mp4'\n",
    "    if kind == \"error_on_purpose\":\n",
    "        return \"wrongful path\"\n",
    "    else:\n",
    "        raise ValueError(f\"Passed Argument kind must bei in ['youtube', 'local', 'error_on_purpose'] but was: {kind}\")\n",
    "\n",
    "def youtube_stream(yt_link):\n",
    "    # Load the video from YouTube\n",
    "    yt_video = YouTube(yt_link)\n",
    "    stream = yt_video.streams.get_highest_resolution() \n",
    "    stream.download()\n",
    "    return cv2.VideoCapture(stream.default_filename)\n",
    "\n",
    "def local_stream(local_path):\n",
    "    return cv2.VideoCapture(local_path)\n",
    "\n",
    "def get_stream(path):\n",
    "    # Check if the string is a YouTube link\n",
    "    if re.match(r'(https?://)?(www\\.)?(youtube\\.com|youtu\\.?be)/.+$', path):\n",
    "        return youtube_stream(path)\n",
    "    # Check if the string is a local path\n",
    "    elif os.path.isfile(path):\n",
    "        return local_stream(path)\n",
    "    # Check if the path is a local file path but no file is found\n",
    "    elif os.path.exists(path):\n",
    "        raise ValueError(f\"File not found at path: {path}\")\n",
    "    # If it's neither a local path nor a YouTube link, raise an error\n",
    "    else:\n",
    "        raise ValueError(\"The input string is neither a local path nor a YouTube link.\")\n",
    "    \n",
    "def load_emotion_classifier():\n",
    "    return load_model(\"../models/emotion_model.hdf5\", compile=False)\n",
    "\n",
    "def preprocess_face(face, input_face_size):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_RGB2GRAY)  # Convert the face to grayscale\n",
    "    face = cv2.resize(face, (input_face_size[1], input_face_size[0]))  # Swap width and height\n",
    "    face = face.astype('float32') / 255.0\n",
    "    face = np.expand_dims(face, axis=-1)  # Add an additional dimension for grayscale channel\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    return face\n",
    "\n",
    "def print_debug_report(operating_results):    \n",
    "    print(f'{operating_results[\"analyzed_emotions\"]} faces found in {operating_results[\"analyzed_frames\"]} frames.')\n",
    "    print(f'{operating_results[\"frames_without_faces\"]} frames had no face detected ({operating_results[\"frames_without_faces_ratio\"]}%).')\n",
    "    print(f'Stopped operations after around {operating_results[\"processed_video_time\"]} seconds into the video.')\n",
    "    print(f'Execution time: {operating_results[\"runtime\"]} seconds, processing (roughly) {round(operating_results[\"processed_video_time\"]/operating_results[\"runtime\"],2)} seconds of video per second of execution')\n",
    "\n",
    "def initialize_face_detector(model_type):\n",
    "    if model_type == 'haarcascade':\n",
    "        return cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    elif model_type == \"MTCNN\":\n",
    "        return MTCNN(keep_all=True, post_process=False, margin=20)\n",
    "    else:\n",
    "        raise ValueError(\"By now, only Haarcascade is implemented.\")\n",
    "\n",
    "def preprocess_frame_for_face_detection_haarcascade(frame):\n",
    "    return cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def preprocess_frame_for_emotion_detection(frame):\n",
    "    return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def normalize_boxes_mtcnn(boxes):\n",
    "    \"\"\"\n",
    "    Normalize the bounding box coordinates from MTCNN to numpy indexing format.\n",
    "    Output format: np.array(y_min, y_max, x_min, x_max)\n",
    "    \"\"\"\n",
    "    normalized_boxes = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box.astype(int)\n",
    "        normalized_boxes.append([y_min, y_max, x_min, x_max])\n",
    "    return np.array(normalized_boxes)\n",
    "\n",
    "def normalize_boxes_cv2(boxes):\n",
    "    \"\"\"\n",
    "    Normalize the bounding box coordinates from OpenCV's format to numpy indexing format.\n",
    "    Output format: np.array(y_min, y_max, x_min, x_max)\n",
    "    \"\"\"\n",
    "    normalized_boxes = []\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "        normalized_boxes.append([y, y+h, x, x+w])\n",
    "    return np.array(normalized_boxes)\n",
    "\n",
    "def detect_faces(frame, face_detector, model_type = 'haarcascade'):\n",
    "    if model_type == 'haarcascade':\n",
    "        frame_pp = preprocess_frame_for_face_detection_haarcascade(frame)\n",
    "        boxes =  face_detector.detectMultiScale(frame_pp, scaleFactor = 1.3, minNeighbors = 3)\n",
    "        if boxes is None:\n",
    "            return None\n",
    "        else:\n",
    "            return normalize_boxes_cv2(boxes)\n",
    "    elif model_type == \"MTCNN\":\n",
    "        # No preprocessing needed for VideoCapture Frame\n",
    "        boxes, _ = face_detector.detect(frame)\n",
    "        if boxes is None:\n",
    "            return None\n",
    "        else:\n",
    "            return normalize_boxes_mtcnn(boxes)\n",
    "        #return [(x['box'][1], x['box'][0] + x['box'][2], x['box'][1] + x['box'][3], x['box'][0]) for x in bounding_boxes]\n",
    "    #elif model_type == \"fast_MTCNN\"\n",
    "    # https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49\n",
    "    else:\n",
    "        raise ValueError(\"By now, only Haarcascade is implemented.\")\n",
    "    \n",
    "def get_ordered_emotions():\n",
    "    return ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "def emotions_probability(frame, face_location, emotion_classifier):\n",
    "    frame_pp = preprocess_frame_for_emotion_detection(frame)\n",
    "    y_min, y_max, x_min, x_max = face_location\n",
    "    face = frame_pp[y_min:y_max, x_min:x_max]\n",
    "    face = preprocess_face(face, input_face_size=emotion_classifier.input_shape[1:3])\n",
    "    prob = emotion_classifier.predict(face)[0]  # check for underscore\n",
    "    return prob\n",
    "\n",
    "\n",
    "def output_video(video, filename):\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    return cv2.VideoWriter(filename, fourcc, 10, (width,height))\n",
    "\n",
    "def get_face(frame, face_location):\n",
    "    # Extract the face region from the frame\n",
    "    #(top, right, bottom, left) = face_location\n",
    "    (y_min, y_max, x_min, x_max) = face_location\n",
    "    face_region = frame[x_min:x_max, y_min:y_max]\n",
    "    face = cv2.cvtColor(face_region, cv2.COLOR_RGB2GRAY)  # Convert the face to grayscale\n",
    "    face_region = cv2.resize(face_region, (256, 256))    \n",
    "    face = face.astype('float32') / 255.0\n",
    "    face = np.expand_dims(face, axis=-1)  # Add an additional dimension for grayscale channel\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    if face.dtype != \"uint8\":\n",
    "        # Find the maximum value based on the current image depth\n",
    "        image_max_value = 2 ** (8 * face.itemsize) - 1\n",
    "        # Convert the image to 8-bit\n",
    "        face = cv2.convertScaleAbs(face, alpha=(255.0 / image_max_value))\n",
    "\n",
    "\n",
    "    return face\n",
    "\n",
    "    \n",
    "def get_face_encoding(face_region):\n",
    "    face_encoding = face_recognition.face_encodings(cv2.cvtColor(face_region, cv2.COLOR_BGR2RGB))\n",
    "    return face_encoding\n",
    "\n",
    "# Initialize known_faces dictionary and character_id\n",
    "character_id = 1\n",
    "face_ids = []\n",
    "known_faces = {}\n",
    "known_faces_list = [known_faces]\n",
    "threshold = 0.6\n",
    "\n",
    "def assign_character_id(face_region, known_faces, threshold, character_id = 1):\n",
    "    current_face_encoding = get_face_encoding(face_region)\n",
    "\n",
    "    \"\"\"if len(current_face_encoding) == 0:         \n",
    "        return None, known_faces\n",
    "    \"\"\"\n",
    "    \n",
    "    # If known_faces is empty, add the current face encoding and assign the first character ID\n",
    "    if not known_faces:\n",
    "        # Add the current face encoding to the dictionary with the first ID\n",
    "        known_faces[character_id] = current_face_encoding\n",
    "        return character_id, known_faces\n",
    "    \n",
    "\n",
    "    # Compare the current face encoding with the known faces\n",
    "    for char_id, face_encoding in known_faces.items():\n",
    "        match = face_recognition.compare_faces([np.asarray(face_encoding)], np.asarray(current_face_encoding), tolerance=threshold)\n",
    "        if match:\n",
    "            # If a match is found, return the character ID\n",
    "            return char_id, known_faces\n",
    "\n",
    "    # No match found, add the face to the list of known faces and assign a new character ID\n",
    "    character_id += 1\n",
    "    known_faces[character_id] = current_face_encoding\n",
    "\n",
    "    return character_id, known_faces\n",
    "\n",
    "\n",
    "def get_overview_df(emotions, character, frame_info, frame_info_cols):\n",
    "    assert len(frame_info[0]) == len(frame_info_cols), \\\n",
    "        f\"Number of columns in frame_info and number of passed names in frame_info_cols is not the same: {len(frame_info[0])} != {len(frame_info_cols)}.\"\n",
    "\n",
    "    df_emotions = pd.DataFrame(emotions, columns=get_ordered_emotions())\n",
    "    df_character = pd.DataFrame(character_ids, columns='character_id')\n",
    "    df_frame_info = pd.DataFrame(frame_info, columns=frame_info_cols)\n",
    "    df_all_info = pd.concat([df_emotions, df_character, df_frame_info], axis=1)\n",
    "    return df_all_info\n",
    "\n",
    "def get_plottable_df(emotions, character, frame_info, frame_info_cols):\n",
    "\n",
    "    assert len(frame_info[0]) == len(frame_info_cols), \\\n",
    "        f\"Number of columns in frame_info and number of passed names in frame_info_cols is not the same: {len(frame_info[0])} != {len(frame_info_cols)}.\"\n",
    "\n",
    "    df_emotions = pd.DataFrame(emotions, columns=get_ordered_emotions())\n",
    "    df_frame_info = pd.DataFrame(frame_info, columns=frame_info_cols)\n",
    "    df_character = pd.DataFrame(character_ids, columns='character_id')\n",
    "\n",
    "    df_all_info = pd.concat([df_emotions, df_character, df_frame_info], axis=1)\n",
    "    df_plotting = pd.melt(df_all_info, id_vars=frame_info_cols, value_vars=get_ordered_emotions(), var_name='emotion', value_name='probability')\n",
    "    return df_plotting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG: start_time = time.time()\n",
    "\n",
    "# Define video path\n",
    "path = get_path('youtube')\n",
    "\n",
    "# Set the number of frames to skip\n",
    "frames_to_skip = 1\n",
    "\n",
    "# Get Video as cv2.VideoCapture\n",
    "# Can access Youtube Video or local file\n",
    "video = get_stream(path)\n",
    "\n",
    "# Initialize the face detection model\n",
    "#model_type = \"MTCNN\"\n",
    "model_type = \"haarcascade\"\n",
    "face_detector = initialize_face_detector(model_type)\n",
    "\n",
    "\n",
    "# Initialize the emotion detection model\n",
    "emotion_classifier = load_emotion_classifier()\n",
    "\n",
    "# Initialize lists to store emotions and frame_info\n",
    "emotions = []\n",
    "frame_info = []\n",
    "face_ids = []\n",
    "\n",
    "face_embeddings = {}\n",
    "\n",
    "\n",
    "# Initialize counters\n",
    "frames_without_faces_counter = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 10:23:24.029487: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m emotions\u001b[39m.\u001b[39mappend(prob)\n\u001b[1;32m     45\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(face_locations):\n\u001b[0;32m---> 46\u001b[0m         face_encoding \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39;49mface_encodings(face, [face_locations[i]])[\u001b[39m0\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[39mcontinue\u001b[39;00m            \n",
      "File \u001b[0;32m~/neuefische/NeuralXpresso/.venv/lib/python3.9/site-packages/face_recognition/api.py:213\u001b[0m, in \u001b[0;36mface_encodings\u001b[0;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mface_encodings\u001b[39m(face_image, known_face_locations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, num_jitters\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msmall\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m    Given an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m    :return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     raw_landmarks \u001b[39m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m [np\u001b[39m.\u001b[39marray(face_encoder\u001b[39m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[39mfor\u001b[39;00m raw_landmark_set \u001b[39min\u001b[39;00m raw_landmarks]\n",
      "File \u001b[0;32m~/neuefische/NeuralXpresso/.venv/lib/python3.9/site-packages/face_recognition/api.py:165\u001b[0m, in \u001b[0;36m_raw_face_landmarks\u001b[0;34m(face_image, face_locations, model)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msmall\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    163\u001b[0m     pose_predictor \u001b[39m=\u001b[39m pose_predictor_5_point\n\u001b[0;32m--> 165\u001b[0m \u001b[39mreturn\u001b[39;00m [pose_predictor(face_image, face_location) \u001b[39mfor\u001b[39;00m face_location \u001b[39min\u001b[39;00m face_locations]\n",
      "File \u001b[0;32m~/neuefische/NeuralXpresso/.venv/lib/python3.9/site-packages/face_recognition/api.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msmall\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    163\u001b[0m     pose_predictor \u001b[39m=\u001b[39m pose_predictor_5_point\n\u001b[0;32m--> 165\u001b[0m \u001b[39mreturn\u001b[39;00m [pose_predictor(face_image, face_location) \u001b[39mfor\u001b[39;00m face_location \u001b[39min\u001b[39;00m face_locations]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "writer = output_video(video, filename='Output_video.mp4') # ADDED THIS!!\n",
    "character_ids= []\n",
    "\n",
    "\n",
    "person_counter = 0\n",
    "known_face_encodings = []\n",
    "character_ids = []\n",
    "FACE_DISTANCE_THRESHOLD = 0.6\n",
    "\n",
    "# Dictionary that will store the known face encodings for each character ID\n",
    "character_faces = {}\n",
    "individual_id_counter = 0\n",
    "\n",
    "\n",
    "# Loop through each frame of the video\n",
    "while True:\n",
    "\n",
    "    # Read the next frame from the video\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    # Check if the frame was successfully read\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Increment the frame counter\n",
    "    current_frame_nr = int(video.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "\n",
    "    # Skip frames based on the frames_to_skip parameter\n",
    "    if current_frame_nr % frames_to_skip != 0:\n",
    "        continue\n",
    "    \n",
    "    # Find faces within a frame and return list of coordinates of bounding boxes\n",
    "    face_locations = detect_faces(frame, face_detector, model_type)\n",
    "\n",
    "    # Check if any faces were found\n",
    "    if face_locations is None:\n",
    "        frames_without_faces_counter += 1\n",
    "        continue\n",
    "\n",
    "    for i, face_location in enumerate(face_locations):\n",
    "        face = get_face(frame, face_location)\n",
    "        prob = emotions_probability(frame, face_location, emotion_classifier)\n",
    "        emotions.append(prob)\n",
    "\n",
    "        if i < len(face_locations):\n",
    "                face_encoding = face_recognition.face_encodings(face, [face_locations[i]])[0]\n",
    "        else:\n",
    "            continue            \n",
    "        current_individual_ids = []\n",
    "\n",
    "        # Compare the current face encoding with the existing face embeddings\n",
    "        for individual_id, individual_face_encoding in face_embeddings.items():\n",
    "            if face_recognition.compare_faces([individual_face_encoding], face_encoding)[0]:\n",
    "                current_individual_ids.append(individual_id)\n",
    "                break\n",
    "        else:\n",
    "            individual_id_counter += 1\n",
    "            current_individual_ids.append(individual_id_counter)\n",
    "            face_embeddings[individual_id_counter] = face_encoding    \n",
    "        \n",
    "        # Add the current timestamp (milliseconds) and probabilities of emotions to the frame_description list\n",
    "        # current implementation to prepare for tuple-wise operation, change frame later to character_nr\n",
    "        \n",
    "        frame_info.append(\n",
    "            (round(video.get(cv2.CAP_PROP_POS_MSEC) / 1000, 2),\n",
    "                current_frame_nr\n",
    "                )\n",
    "        )    \n",
    "\n",
    "        max_emotion, max_prob = np.argmax(prob), np.max(prob)\n",
    "        emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "        emotion_text = emotion_labels[max_emotion]\n",
    "\n",
    "\n",
    "        # Assign character IDs to each face in the frame\n",
    "        character_id = assign_character_id(frame, [face_location])\n",
    "        character_ids.append(character_id)\n",
    "\n",
    "\n",
    "        for y_min, y_max, x_min, x_max in face_locations:\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Prob: {max_prob:.1%}\", (x_min, y_max + 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "            cv2.putText(frame, f\"{emotion_text}\", (x_min, y_max + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "            # Add the character ID as an annotation to the frame\n",
    "            cv2.putText(frame, f\"Character ID: {current_individual_ids[i]}\", (x_min, y_min - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)\n",
    "\n",
    "\n",
    "\n",
    "    writer.write(frame) # ADDED THIS!!!\n",
    "\n",
    "    cv2.imshow(\"Faces found\", frame)\n",
    "\n",
    "    # Wait for Esc key to stop\n",
    "    if cv2.waitKey(20) == ord('q'):\n",
    "        break\n",
    "\n",
    "    if DEBUG:\n",
    "        # For debugging reasons, we stop when we have 1000 emotion values\n",
    "        if len(emotions) > debug_params['max_emotions']:\n",
    "            break\n",
    "\n",
    "# Release the video and close the window\n",
    "video.release() \n",
    "\n",
    "writer.release() # ADDED THIS!!!\n",
    "\n",
    "# De-allocate any associated memory usage\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if DEBUG: end_time = time.time()\n",
    "\n",
    "if DEBUG:\n",
    "    # When in DEBUG-mode, print some statistics about the faces and emotions detected\n",
    "    operating_results = {'analyzed_emotions': len(emotions),\n",
    "                         'analyzed_frames':frame_info[-1][1], \n",
    "                         'frames_without_faces':frames_without_faces_counter,\n",
    "                         'frames_without_faces_ratio': round(100*frames_without_faces_counter/frame_info[-1][1],2),\n",
    "                         'processed_video_time': round(frame_info[-1][0] / 1000 ,2),\n",
    "                         'runtime': round(end_time - start_time,2)}\n",
    "    \n",
    "    print_debug_report(operating_results)\n",
    "    \n",
    "df_plotting = get_plottable_df(emotions, character, frame_info, frame_info_cols=['pos_sec', 'frame'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "292d9171ab7c5408b6324c6043caf24aa9a11213294e6785916c17bd69f18dee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
