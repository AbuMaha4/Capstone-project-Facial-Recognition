{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from facenet_pytorch import MTCNN\n",
    "import face_recognition\n",
    "\n",
    "from pytube import YouTube\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, batch_size=50, skip_frames=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.skip_frames = skip_frames\n",
    "        self.frames_without_faces = 0\n",
    "\n",
    "    def load_video_from_youtube(self, yt_link):\n",
    "        self.yt_link = yt_link\n",
    "        # Download the YouTube video and get the highest resolution stream\n",
    "        yt_video = YouTube(self.yt_link)\n",
    "        stream = yt_video.streams.get_highest_resolution()\n",
    "\n",
    "        # Open the video stream using OpenCV\n",
    "        self.video = cv2.VideoCapture(stream.url)\n",
    "\n",
    "        # Get available and used resolution - Debugging\n",
    "        #self.available_resolutions = [streams.resolution for streams in yt_video.streams.filter(type=\"video\", progressive=True)]\n",
    "        #self.used_resolution = stream.resolution\n",
    "\n",
    "        # Get the number of frames in the video\n",
    "        self.frame_count = int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Get the frame rate of the video\n",
    "        self.fps = int(self.video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "        # Get the height and width of the video frames\n",
    "        self.height = int(self.video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.width = int(self.video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "        # Calculate the total number of frames to process after skipping frames\n",
    "        self.total_frames = len(range(0, self.frame_count, self.skip_frames))\n",
    "\n",
    "        # Calculate the number of batches required to process all the frames\n",
    "        self.num_batches = int(np.ceil(self.total_frames / self.batch_size))\n",
    "\n",
    "    def get_batches(self):\n",
    "        # Initialize an empty numpy array to hold the frames\n",
    "        frames = np.empty((self.batch_size, self.height, self.width, 3), np.dtype('uint8'))\n",
    "\n",
    "        self.frames_read = 0\n",
    "\n",
    "        # Read the frames in batches and fill up the numpy array\n",
    "        for batch_start in range(0, self.frame_count, self.batch_size * self.skip_frames):\n",
    "            batch_end = min(batch_start + (self.batch_size * self.skip_frames), self.frame_count)\n",
    "            batch_index = 0\n",
    "\n",
    "            for i in range(batch_start, batch_end):\n",
    "                ret, frame = self.video.read()\n",
    "                self.frames_read +=1\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                if i % self.skip_frames == 0:\n",
    "                    frames[batch_index] = frame\n",
    "                    batch_index += 1\n",
    "\n",
    "            # Resize the numpy array to fit the actual number of frames in the batch\n",
    "            if batch_index < self.batch_size:\n",
    "                frames = frames[:batch_index]\n",
    "\n",
    "            # Yield the current batch of frames\n",
    "            yield frames\n",
    "\n",
    "        # Release the video stream\n",
    "        self.video.release()\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self, detection_type=\"face_recognition\"):\n",
    "        self.detection_type = detection_type\n",
    "\n",
    "        if self.detection_type == \"MTCNN\":\n",
    "            # Initialize the MTCNN face detector\n",
    "            self.face_detector = MTCNN(keep_all=True, post_process=False, margin=20)\n",
    "        elif self.detection_type == \"haarcascade\":\n",
    "            # Load the Haar Cascade face detector\n",
    "            self.face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        elif self.detection_type == \"face_recognition\":\n",
    "            # Package face_detector doesn't require object initialization\n",
    "            self.face_detector = None\n",
    "        else: \n",
    "            raise ValueError(\"Choose one of the implemented models, ya cunt!\")\n",
    "\n",
    "\n",
    "    def detect_faces(self, frame):\n",
    "        if self.detection_type == 'haarcascade':\n",
    "            frame_pp = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            boxes = self.face_detector.detectMultiScale(frame_pp, scaleFactor=1.3, minNeighbors=3)\n",
    "            \n",
    "        elif self.detection_type == 'MTCNN':\n",
    "            # No preprocessing needed\n",
    "            boxes, _ = self.face_detector.detect(frame)\n",
    "            \n",
    "        elif self.detection_type == 'face_recognition':\n",
    "            cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Package face_detector directly return boxes without object initialization\n",
    "            boxes = face_recognition.face_locations(frame_pp, number_of_times_to_upsample=1)\n",
    "        \n",
    "        if (boxes is None) or (len(boxes) == 0):\n",
    "            return [] \n",
    "\n",
    "        # Convert boxes to normalized format\n",
    "        norm_boxes = self.get_norm_boxes(boxes)\n",
    "\n",
    "        valid_boxes = []\n",
    "        for box in norm_boxes: \n",
    "            box_rearranged = np.array([box[0],box[3], box[1], box[2]])\n",
    "            face_landmark = face_recognition.face_landmarks(frame, [box_rearranged])[0]\n",
    "            if FaceDetector.valid_landmarks(face_landmark):\n",
    "                valid_boxes.append(box) \n",
    "\n",
    "        return valid_boxes\n",
    "\n",
    "    def get_norm_boxes(self, boxes):\n",
    "        \"\"\"\n",
    "        Normalize the bounding box coordinates from MTCNN to numpy indexing format.\n",
    "        Output format: np.array(y_min, y_max, x_min, x_max)\n",
    "        \"\"\"\n",
    "        normalized_box = []\n",
    "\n",
    "        if self.detection_type == 'haarcascade':\n",
    "            for box in boxes:\n",
    "                x, y, w, h = box\n",
    "                normalized_box.append([y, y+h, x, x+w])\n",
    "            return np.array(normalized_box)    \n",
    "\n",
    "        elif self.detection_type == \"MTCNN\":\n",
    "            for box in boxes:\n",
    "                x_min, y_min, x_max, y_max = box.astype(int)\n",
    "                normalized_box.append([y_min, y_max, x_min, x_max])\n",
    "            return np.array(normalized_box)\n",
    "\n",
    "        elif self.detection_type =='face_recognition':\n",
    "            for box in boxes:\n",
    "                y_min, x_max, y_max, x_min = box\n",
    "                normalized_box.append([y_min, y_max, x_min, x_max])\n",
    "            return np.array(normalized_box)    \n",
    "    \n",
    "    @staticmethod\n",
    "    def valid_landmarks(face_landmark):\n",
    "        left_eye = face_landmark['left_eye']\n",
    "        right_eye = face_landmark['right_eye']\n",
    "        mouth = face_landmark['top_lip'] + face_landmark['bottom_lip']\n",
    "\n",
    "        left_eye_center = np.mean(left_eye, axis=0)\n",
    "        right_eye_center = np.mean(right_eye, axis=0)\n",
    "        mouth_center = np.mean(mouth, axis=0)\n",
    "\n",
    "        eye_distance = np.linalg.norm(right_eye_center - left_eye_center)\n",
    "        mouth_to_eye_distance = np.linalg.norm(mouth_center - left_eye_center)\n",
    "\n",
    "        return mouth_to_eye_distance > eye_distance * 0.8\n",
    "\n",
    "class EmotionDetector:\n",
    "    def __init__(self):\n",
    "        self.emotion_detector = load_model(\"../models/emotion_model.hdf5\", compile=False)\n",
    "        self.emotion_categories = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    \n",
    "    def predict(self, frame, box):\n",
    "\n",
    "        gray_cropped_face = EmotionDetector.crop_face(box, frame)\n",
    "\n",
    "        prob = self.emotion_detector.predict(gray_cropped_face)[0]  # check for underscore\n",
    "        return prob\n",
    "    \n",
    "    @staticmethod\n",
    "    def crop_face(box, frame):\n",
    "        face = frame[box[0]:box[1], box[2]:box[3]]\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        face = cv2.resize(face, (64, 64))\n",
    "        face = face.astype('float32')/ 255\n",
    "        face = np.expand_dims(face, axis=-1)\n",
    "        face = np.expand_dims(face, axis=0)\n",
    "\n",
    "        return face\n",
    "    \n",
    "class PersonIdentifier:\n",
    "    def __init__(self, threshold = 0.6):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def assignID(self, frame, box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 363ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:22:01.560797: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:4199\u001b[0m, in \u001b[0;36mGraph._get_op_def\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   4198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 4199\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op_def_cache[\u001b[39mtype\u001b[39;49m]\n\u001b[1;32m   4200\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Const'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m face_boxes: \n\u001b[0;32m---> 23\u001b[0m         emotions\u001b[39m.\u001b[39mappend((frame_nr, my_emotion_detector\u001b[39m.\u001b[39;49mpredict(frame, box)))\n\u001b[1;32m     25\u001b[0m     frame_nr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(batch)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 171\u001b[0m, in \u001b[0;36mEmotionDetector.predict\u001b[0;34m(self, frame, box)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, frame, box):\n\u001b[1;32m    169\u001b[0m     gray_cropped_face \u001b[39m=\u001b[39m EmotionDetector\u001b[39m.\u001b[39mcrop_face(box, frame)\n\u001b[0;32m--> 171\u001b[0m     prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49memotion_detector\u001b[39m.\u001b[39;49mpredict(gray_cropped_face)[\u001b[39m0\u001b[39m]  \u001b[39m# check for underscore\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[39mreturn\u001b[39;00m prob\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/keras/engine/training.py:2317\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2309\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2310\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2311\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2314\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2315\u001b[0m         )\n\u001b[0;32m-> 2317\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   2318\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   2319\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2320\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   2321\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2322\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2323\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2324\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2325\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2326\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2327\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m   2328\u001b[0m )\n\u001b[1;32m   2330\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:1579\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1578\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1579\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:1259\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1258\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1260\u001b[0m     x,\n\u001b[1;32m   1261\u001b[0m     y,\n\u001b[1;32m   1262\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1263\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1264\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1265\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1266\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1267\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1268\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1269\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1270\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1271\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1272\u001b[0m )\n\u001b[1;32m   1274\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:345\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         flat_dataset \u001b[39m=\u001b[39m flat_dataset\u001b[39m.\u001b[39mshuffle(\u001b[39m1024\u001b[39m)\u001b[39m.\u001b[39mrepeat(epochs)\n\u001b[1;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m--> 345\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mflat_map(slice_batch_indices)\n\u001b[1;32m    347\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:2337\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_map\u001b[39m(\u001b[39mself\u001b[39m, map_func, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2305\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Maps `map_func` across this dataset and flattens the result.\u001b[39;00m\n\u001b[1;32m   2306\u001b[0m \n\u001b[1;32m   2307\u001b[0m \u001b[39m  The type signature is:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2335\u001b[0m \u001b[39m    A new `Dataset` with the transformation applied as described above.\u001b[39;00m\n\u001b[1;32m   2336\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2337\u001b[0m   \u001b[39mreturn\u001b[39;00m FlatMapDataset(\u001b[39mself\u001b[39;49m, map_func, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:5583\u001b[0m, in \u001b[0;36mFlatMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m   5581\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m   5582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[0;32m-> 5583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[1;32m   5584\u001b[0m     map_func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(), dataset\u001b[39m=\u001b[39;49minput_dataset)\n\u001b[1;32m   5585\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure, DatasetSpec):\n\u001b[1;32m   5586\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   5587\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   5588\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m_get_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure)\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:263\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    257\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 263\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m    264\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:226\u001b[0m, in \u001b[0;36mTracingCompiler.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    218\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39m      `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m   concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[1;32m    227\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    228\u001b[0m   concrete_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    229\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:192\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mvalidate_inputs_with_signature(args, kwargs)\n\u001b[1;32m    191\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 192\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[1;32m    193\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    194\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m    195\u001b[0m       concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:157\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    155\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:360\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m    358\u001b[0m   args, kwargs \u001b[39m=\u001b[39m generalized_func_key\u001b[39m.\u001b[39m_placeholder_value()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(args, kwargs)\n\u001b[1;32m    362\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:284\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    280\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m    281\u001b[0m ]\n\u001b[1;32m    282\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m    283\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 284\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    285\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    287\u001b[0m         args,\n\u001b[1;32m    288\u001b[0m         kwargs,\n\u001b[1;32m    289\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    290\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m    291\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m    292\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    293\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m    294\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    295\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    296\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1283\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1283\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1285\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:240\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    235\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    236\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[1;32m    237\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    238\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    241\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    242\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:171\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    170\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 171\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[1;32m    172\u001b[0m ret \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:326\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__.<locals>.slice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[1;32m    311\u001b[0m \u001b[39mThis step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39m  A Dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    325\u001b[0m num_in_full_batch \u001b[39m=\u001b[39m num_full_batches \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m--> 326\u001b[0m first_k_indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mslice(indices, [\u001b[39m0\u001b[39;49m], [num_in_full_batch])\n\u001b[1;32m    327\u001b[0m first_k_indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m    328\u001b[0m     first_k_indices, [num_full_batches, batch_size]\n\u001b[1;32m    329\u001b[0m )\n\u001b[1;32m    331\u001b[0m flat_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(first_k_indices)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:1163\u001b[0m, in \u001b[0;36mslice\u001b[0;34m(input_, begin, size, name)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mslice\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1112\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mslice\u001b[39m(input_, begin, size, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1114\u001b[0m   \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Extracts a slice from a tensor.\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \n\u001b[1;32m   1117\u001b[0m \u001b[39m  See also `tf.strided_slice`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39m    A `Tensor` the same type as `input_`.\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1163\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49m_slice(input_, begin, size, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:9589\u001b[0m, in \u001b[0;36m_slice\u001b[0;34m(input, begin, size, name)\u001b[0m\n\u001b[1;32m   9587\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m   9588\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m-> 9589\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[1;32m   9590\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mSlice\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m, begin\u001b[39m=\u001b[39;49mbegin, size\u001b[39m=\u001b[39;49msize, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   9591\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[1;32m   9592\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:777\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[39mwith\u001b[39;00m g\u001b[39m.\u001b[39mas_default(), ops\u001b[39m.\u001b[39mname_scope(name) \u001b[39mas\u001b[39;00m scope:\n\u001b[1;32m    776\u001b[0m   \u001b[39mif\u001b[39;00m fallback:\n\u001b[0;32m--> 777\u001b[0m     _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n\u001b[1;32m    778\u001b[0m                            keywords, default_type_attr_map, attrs, inputs,\n\u001b[1;32m    779\u001b[0m                            input_types)\n\u001b[1;32m    780\u001b[0m     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n\u001b[1;32m    781\u001b[0m                            default_type_attr_map, attrs)\n\u001b[1;32m    782\u001b[0m     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:550\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    544\u001b[0m       values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    545\u001b[0m           values,\n\u001b[1;32m    546\u001b[0m           name\u001b[39m=\u001b[39minput_arg\u001b[39m.\u001b[39mname,\n\u001b[1;32m    547\u001b[0m           as_ref\u001b[39m=\u001b[39minput_arg\u001b[39m.\u001b[39mis_ref,\n\u001b[1;32m    548\u001b[0m           preferred_dtype\u001b[39m=\u001b[39mdefault_dtype)\n\u001b[1;32m    549\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(\n\u001b[1;32m    551\u001b[0m         values,\n\u001b[1;32m    552\u001b[0m         name\u001b[39m=\u001b[39;49minput_arg\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    553\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    554\u001b[0m         as_ref\u001b[39m=\u001b[39;49minput_arg\u001b[39m.\u001b[39;49mis_ref,\n\u001b[1;32m    555\u001b[0m         preferred_dtype\u001b[39m=\u001b[39;49mdefault_dtype)\n\u001b[1;32m    556\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    557\u001b[0m   \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1636\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1627\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1628\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1629\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1632\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1633\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1635\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1638\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1639\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m dtype_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mtensor_value\u001b[39m.\u001b[39mtensor\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    288\u001b[0m attrs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: tensor_value, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: dtype_value}\n\u001b[0;32m--> 289\u001b[0m const_tensor \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    290\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mConst\u001b[39;49m\u001b[39m\"\u001b[39;49m, [], [dtype_value\u001b[39m.\u001b[39;49mtype], attrs\u001b[39m=\u001b[39;49mattrs, name\u001b[39m=\u001b[39;49mname)\u001b[39m.\u001b[39moutputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m op_callbacks\u001b[39m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[1;32m    293\u001b[0m   \u001b[39m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[1;32m    294\u001b[0m   \u001b[39m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m   callback_outputs \u001b[39m=\u001b[39m op_callbacks\u001b[39m.\u001b[39minvoke_op_callbacks(\n\u001b[1;32m    296\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[39m=\u001b[39mname, graph\u001b[39m=\u001b[39mg)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:749\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    747\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[1;32m    748\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[0;32m--> 749\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    750\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[1;32m    751\u001b[0m     compute_device)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3798\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3795\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[1;32m   3796\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[1;32m   3797\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[0;32m-> 3798\u001b[0m   ret \u001b[39m=\u001b[39m Operation(\n\u001b[1;32m   3799\u001b[0m       node_def,\n\u001b[1;32m   3800\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   3801\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m   3802\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[1;32m   3803\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[1;32m   3804\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[1;32m   3805\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[1;32m   3806\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[1;32m   3807\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[1;32m   3808\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2106\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2103\u001b[0m     control_input_ops\u001b[39m.\u001b[39mappend(control_op)\n\u001b[1;32m   2105\u001b[0m \u001b[39m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[0;32m-> 2106\u001b[0m c_op \u001b[39m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[1;32m   2107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_from_c_op(c_op\u001b[39m=\u001b[39mc_op, g\u001b[39m=\u001b[39mg)\n\u001b[1;32m   2109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_op \u001b[39m=\u001b[39m original_op\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1931\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a TF_Operation.\u001b[39;00m\n\u001b[1;32m   1915\u001b[0m \n\u001b[1;32m   1916\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[39m  A wrapped TF_Operation*.\u001b[39;00m\n\u001b[1;32m   1929\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1930\u001b[0m \u001b[39mif\u001b[39;00m op_def \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1931\u001b[0m   op_def \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49m_get_op_def(node_def\u001b[39m.\u001b[39;49mop)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1932\u001b[0m \u001b[39m# TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.\u001b[39;00m\n\u001b[1;32m   1933\u001b[0m \u001b[39m# Refactor so we don't have to do this here.\u001b[39;00m\n\u001b[1;32m   1934\u001b[0m inputs \u001b[39m=\u001b[39m _reconstruct_sequence_inputs(op_def, inputs, node_def\u001b[39m.\u001b[39mattr)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:4202\u001b[0m, in \u001b[0;36mGraph._get_op_def\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   4200\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m   4201\u001b[0m   \u001b[39mwith\u001b[39;00m c_api_util\u001b[39m.\u001b[39mtf_buffer() \u001b[39mas\u001b[39;00m buf:\n\u001b[0;32m-> 4202\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_c_graph\u001b[39m.\u001b[39;49mget() \u001b[39mas\u001b[39;00m c_graph:\n\u001b[1;32m   4203\u001b[0m       pywrap_tf_session\u001b[39m.\u001b[39mTF_GraphGetOpDef(c_graph, compat\u001b[39m.\u001b[39mas_bytes(\u001b[39mtype\u001b[39m),\n\u001b[1;32m   4204\u001b[0m                                          buf)\n\u001b[1;32m   4205\u001b[0m     data \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_GetBuffer(buf)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/contextlib.py:263\u001b[0m, in \u001b[0;36mcontextmanager.<locals>.helper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    262\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhelper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m--> 263\u001b[0m     \u001b[39mreturn\u001b[39;00m _GeneratorContextManager(func, args, kwds)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/contextlib.py:86\u001b[0m, in \u001b[0;36m_GeneratorContextManagerBase.__init__\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_GeneratorContextManagerBase\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Shared functionality for @contextmanager and @asynccontextmanager.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, func, args, kwds):\n\u001b[1;32m     87\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     88\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds \u001b[39m=\u001b[39m func, args, kwds\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'                   # 1 Actor\n",
    "#yt_link = 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "#yt_link = 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "yt_link = 'https://www.youtube.com/watch?v=UECCHwh7bZE'\n",
    "\n",
    "my_test = VideoProcessor(skip_frames=10, batch_size=10000)\n",
    "my_face_detector = FaceDetector(detection_type='haarcascade')\n",
    "my_emotion_detector = EmotionDetector()\n",
    "my_person_identifier = PersonIdentifier()\n",
    "emotions = []\n",
    "frame_nr = 0\n",
    "my_test.load_video_from_youtube(yt_link)\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    for frame in batch:\n",
    "        face_boxes = my_face_detector.detect_faces(frame)\n",
    "        \n",
    "        if len(face_boxes) == 0:\n",
    "            my_test.frames_without_faces += 1\n",
    "            continue\n",
    "        \n",
    "        for box in face_boxes: \n",
    "            emotions.append((frame_nr, my_emotion_detector.predict(frame, box)))\n",
    "            character_ID = my_person_identifier.assignID(frame, box)\n",
    "        frame_nr +=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m my_test\u001b[39m.\u001b[39mload_video_from_youtube(yt_link)\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(my_test\u001b[39m.\u001b[39mget_batches()):\n\u001b[0;32m----> 7\u001b[0m     boxes \u001b[39m=\u001b[39m my_face_detector\u001b[39m.\u001b[39;49mdetect_faces(batch)\n",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m, in \u001b[0;36mFaceDetector.detect_faces\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect_faces\u001b[39m(\u001b[39mself\u001b[39m, frames):\n\u001b[1;32m     88\u001b[0m     \u001b[39m# Detect faces in the frames using the selected face detection model\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetection_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmtcnn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m         \u001b[39m# Use MTCNN face detector\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m         return_boxes, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mface_detector\u001b[39m.\u001b[39;49mdetect(frames)\n\u001b[1;32m     92\u001b[0m         \u001b[39mreturn\u001b[39;00m return_boxes\n\u001b[1;32m     93\u001b[0m         boxes \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[1;32m    314\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[1;32m    315\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[1;32m    317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    320\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:183\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m    180\u001b[0m     batch_boxes\u001b[39m.\u001b[39mappend(boxes[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m    181\u001b[0m     batch_points\u001b[39m.\u001b[39mappend(points[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m--> 183\u001b[0m batch_boxes, batch_points \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(batch_boxes), np\u001b[39m.\u001b[39marray(batch_points)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m batch_boxes, batch_points\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=wo6K1GWEx84'\n",
    "\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "my_test.load_video_from_youtube(yt_link)\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    boxes = my_face_detector.detect_faces(batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(boxlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(batch)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m batchlist\u001b[39m.\u001b[39mappend(batch)\n\u001b[0;32m---> 11\u001b[0m boxes \u001b[39m=\u001b[39m my_face_detector\u001b[39m.\u001b[39;49mdetect_faces(batch)\n\u001b[1;32m     12\u001b[0m boxlist\u001b[39m.\u001b[39mappend(boxes)\n",
      "Cell \u001b[0;32mIn[64], line 91\u001b[0m, in \u001b[0;36mFaceDetector.detect_faces\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect_faces\u001b[39m(\u001b[39mself\u001b[39m, frames):\n\u001b[1;32m     88\u001b[0m     \u001b[39m# Detect faces in the frames using the selected face detection model\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetection_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmtcnn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m         \u001b[39m# Use MTCNN face detector\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m         return_boxes, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mface_detector\u001b[39m.\u001b[39;49mdetect(frames)\n\u001b[1;32m     92\u001b[0m         \u001b[39mreturn\u001b[39;00m return_boxes\n\u001b[1;32m     93\u001b[0m         boxes \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[1;32m    314\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[1;32m    315\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[1;32m    317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    320\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:183\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m    180\u001b[0m     batch_boxes\u001b[39m.\u001b[39mappend(boxes[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m    181\u001b[0m     batch_points\u001b[39m.\u001b[39mappend(points[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m--> 183\u001b[0m batch_boxes, batch_points \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(batch_boxes), np\u001b[39m.\u001b[39marray(batch_points)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m batch_boxes, batch_points\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=wo6K1GWEx84'\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "#my_emotion_detector = EmotionDetector\n",
    "batchlist = []\n",
    "boxlist = []\n",
    "my_test.load_video_from_youtube(yt_link)\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    print(f'{idx}: {len(batch)}')\n",
    "    batchlist.append(batch)\n",
    "    boxes = my_face_detector.detect_faces(batch)\n",
    "    boxlist.append(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 720, 1280, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[ 800.64   ,  155.62999, 1065.035  ,  477.59427]],\n",
      "\n",
      "       [[ 787.31104,  160.48291, 1047.8341 ,  486.15082]],\n",
      "\n",
      "       [[ 736.2914 ,  159.95483,  996.07245,  482.7968 ]],\n",
      "\n",
      "       [[ 724.2704 ,  159.92754,  975.6582 ,  487.37204]],\n",
      "\n",
      "       [[ 715.5035 ,  159.72684,  956.12256,  473.42117]],\n",
      "\n",
      "       [[ 205.15468,  180.71413,  391.8848 ,  428.30347]],\n",
      "\n",
      "       [[ 213.19006,  178.01108,  399.1574 ,  423.1865 ]],\n",
      "\n",
      "       [[ 227.02106,  182.68604,  408.90018,  420.32648]],\n",
      "\n",
      "       [[ 761.26526,  181.73456, 1000.58075,  492.46167]]], dtype=float32), array([[0.9998952 ],\n",
      "       [0.9997003 ],\n",
      "       [0.99998546],\n",
      "       [0.99999976],\n",
      "       [0.9988213 ],\n",
      "       [0.9996246 ],\n",
      "       [0.99990594],\n",
      "       [0.99990046],\n",
      "       [0.99999857]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "face_detector_mtcnn = MTCNN()\n",
    "a = face_detector_mtcnn.detect(batch[3:12])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1, 4)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import face_detection\n",
    "# Initialize detector\n",
    "detector = face_detection.build_detector(\"DSFDDetector\", confidence_threshold=.5, nms_iou_threshold=.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting detections\n",
    "detections = detector.detect(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m detections \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39;49mbatched_detect(batch)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/face_detection/base.py:146\u001b[0m, in \u001b[0;36mDetector.batched_detect\u001b[0;34m(self, image, shrink)\u001b[0m\n\u001b[1;32m    144\u001b[0m height, width \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:\u001b[39m3\u001b[39m]\n\u001b[1;32m    145\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_process(image, shrink)\n\u001b[0;32m--> 146\u001b[0m boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batched_detect(image)\n\u001b[1;32m    147\u001b[0m boxes \u001b[39m=\u001b[39m [scale_boxes((height, width), box)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m boxes]\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_detections(boxes)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/face_detection/base.py:126\u001b[0m, in \u001b[0;36mDetector._batched_detect\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batched_detect\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mList[np\u001b[39m.\u001b[39mndarray]:\n\u001b[0;32m--> 126\u001b[0m     boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_detect(image)\n\u001b[1;32m    127\u001b[0m     boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_boxes(boxes)\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_boxes:\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/face_detection/dsfd/detect.py:40\u001b[0m, in \u001b[0;36mDSFDDetector._detect\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m x \u001b[39m=\u001b[39m x[:, [\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m], :, :]\n\u001b[1;32m     39\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_inference):\n\u001b[0;32m---> 40\u001b[0m     boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(\n\u001b[1;32m     41\u001b[0m         x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfidence_threshold, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnms_iou_threshold\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m boxes\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/face_detection/dsfd/face_ssd.py:147\u001b[0m, in \u001b[0;36mSSD.forward\u001b[0;34m(self, x, confidence_threshold, nms_threshold)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m# ResNet152\u001b[39;00m\n\u001b[1;32m    146\u001b[0m conv3_3_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 147\u001b[0m conv4_3_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(conv3_3_x)\n\u001b[1;32m    148\u001b[0m conv5_3_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(conv4_3_x)\n\u001b[1;32m    149\u001b[0m fc7_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(conv5_3_x)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    144\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m--> 146\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    147\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "detections = detector.batched_detect(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting retina-face\n",
      "  Using cached retina_face-0.0.13-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: retina-face\n",
      "Successfully installed retina-face-0.0.13\n"
     ]
    }
   ],
   "source": [
    "!pip install retina-face --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 720, 1280, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/onnx/onnx_importer.cpp:270: error: (-5:Bad argument) Can't read ONNX file: /models/face_detection_yunet_2022mar.onnx in function 'ONNXImporter'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Initialize detector\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m detector \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mFaceDetectorYN\u001b[39m.\u001b[39;49mcreate(\u001b[39m\"\u001b[39;49m\u001b[39m/models/face_detection_yunet_2022mar.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m320\u001b[39;49m, \u001b[39m320\u001b[39;49m))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/onnx/onnx_importer.cpp:270: error: (-5:Bad argument) Can't read ONNX file: /models/face_detection_yunet_2022mar.onnx in function 'ONNXImporter'\n"
     ]
    }
   ],
   "source": [
    "# Initialize detector\n",
    "detector = cv2.FaceDetectorYN.create(\"/models/face_detection_yunet_2022mar.onnx\", \"\", (320, 320))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input size\n",
    "detector.setInputSize((1280, 720))\n",
    "# Getting detections\n",
    "detections = detector.detect(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 213.83145,  219.39542,  415.32016,  483.84225],\n",
      "       [ 834.7785 ,  208.32353, 1019.23663,  466.85956]], dtype=float32), array([0.99976796, 0.99993265], dtype=float32))\n",
      "(array([[ 224.82489,  210.4985 ,  420.9738 ,  463.00748],\n",
      "       [ 835.11536,  201.44376, 1015.41327,  461.78235]], dtype=float32), array([0.9999089 , 0.99991834], dtype=float32))\n",
      "(array([[ 791.4241 ,  163.10724, 1054.3726 ,  494.60812]], dtype=float32), array([0.9998498], dtype=float32))\n",
      "(array([[ 800.64   ,  155.63005, 1065.035  ,  477.5943 ]], dtype=float32), array([0.9998952], dtype=float32))\n",
      "(array([[ 787.31104,  160.48288, 1047.8341 ,  486.1508 ]], dtype=float32), array([0.9997003], dtype=float32))\n",
      "(array([[736.2914 , 159.95485, 996.07245, 482.7968 ]], dtype=float32), array([0.99998546], dtype=float32))\n",
      "(array([[724.2704 , 159.92757, 975.6582 , 487.37204]], dtype=float32), array([0.99999976], dtype=float32))\n",
      "(array([[715.5035 , 159.7268 , 956.12256, 473.4211 ]], dtype=float32), array([0.9988213], dtype=float32))\n",
      "(array([[205.15466, 180.71411, 391.8848 , 428.30344]], dtype=float32), array([0.9996246], dtype=float32))\n",
      "(array([[213.1901 , 178.01111, 399.15738, 423.18646]], dtype=float32), array([0.99990594], dtype=float32))\n",
      "(array([[227.02107, 182.68604, 408.90018, 420.32645]], dtype=float32), array([0.99990046], dtype=float32))\n",
      "(array([[ 761.26526,  181.73456, 1000.5807 ,  492.46158]], dtype=float32), array([0.99999857], dtype=float32))\n",
      "(array([[ 755.8671 ,  181.15637,  985.30756,  477.73108],\n",
      "       [1106.7161 ,  577.9526 , 1169.9537 ,  652.95416]], dtype=float32), array([0.99981326, 0.7708278 ], dtype=float32))\n",
      "(array([[753.9056 , 180.78487, 974.0696 , 482.5361 ]], dtype=float32), array([0.99957734], dtype=float32))\n",
      "(array([[ 754.16943,  208.91489,  968.6512 ,  505.67026],\n",
      "       [1099.8883 ,  589.2897 , 1174.5776 ,  684.39526]], dtype=float32), array([0.9983754 , 0.95600593], dtype=float32))\n",
      "(array([[185.9277 , 211.35156, 373.66745, 447.26688]], dtype=float32), array([0.9992199], dtype=float32))\n",
      "(array([[191.53319 , 207.84862 , 378.68674 , 449.3293  ],\n",
      "       [-25.061834,  87.27451 ,  65.49193 , 202.66249 ]], dtype=float32), array([0.99981695, 0.8056566 ], dtype=float32))\n",
      "(array([[239.01352, 199.12521, 424.81085, 442.0615 ]], dtype=float32), array([0.9998908], dtype=float32))\n",
      "(array([[249.1043 , 190.8    , 427.80307, 427.34714],\n",
      "       [891.7605 , 583.742  , 911.19055, 606.6293 ]], dtype=float32), array([0.99971193, 0.81702673], dtype=float32))\n",
      "(array([[ 793.3452 ,  187.82239, 1025.1099 ,  502.68195]], dtype=float32), array([0.9998803], dtype=float32))\n",
      "(array([[ 806.6299 ,  182.14539, 1037.3386 ,  503.12357]], dtype=float32), array([0.9992404], dtype=float32))\n",
      "(array([[ 807.1734 ,  188.74097, 1036.4078 ,  503.49225]], dtype=float32), array([0.9996092], dtype=float32))\n",
      "(array([[235.81143, 179.23465, 411.17484, 415.74368]], dtype=float32), array([0.99999034], dtype=float32))\n",
      "(array([[ 235.18594,  181.55789,  411.30865,  414.63428],\n",
      "       [1114.435  ,  646.7671 , 1160.2509 ,  702.2205 ]], dtype=float32), array([0.9999881, 0.7103028], dtype=float32))\n",
      "(array([[ 229.61168,  173.42664,  402.60376,  409.19217],\n",
      "       [1111.544  ,  646.5156 , 1159.4585 ,  703.74994]], dtype=float32), array([0.99999166, 0.8855344 ], dtype=float32))\n",
      "(array([[228.8537 , 171.57059, 402.047  , 410.2464 ]], dtype=float32), array([0.9999732], dtype=float32))\n",
      "(array([[ 223.4795 ,  176.69102,  394.9144 ,  411.49283],\n",
      "       [1114.0039 ,  646.9772 , 1160.2795 ,  703.5945 ]], dtype=float32), array([0.99999344, 0.7187914 ], dtype=float32))\n",
      "(array([[218.00972, 174.80247, 391.40262, 414.4093 ]], dtype=float32), array([0.99998116], dtype=float32))\n",
      "(array([[ 751.46967,  153.84312, 1034.2013 ,  520.29047]], dtype=float32), array([0.9998987], dtype=float32))\n",
      "(array([[ 780.4861 ,  157.16771, 1063.3136 ,  523.04974]], dtype=float32), array([0.9999727], dtype=float32))\n",
      "(array([[ 787.6143 ,  163.02722, 1077.2628 ,  518.97736]], dtype=float32), array([0.9999999], dtype=float32))\n",
      "(array([[ 778.96625,  157.48637, 1067.3943 ,  521.8188 ],\n",
      "       [ 162.74947,  182.94966,  358.36838,  434.48328]], dtype=float32), array([0.99999106, 0.9982285 ], dtype=float32))\n",
      "(array([[ 169.899  ,  176.23572,  355.5777 ,  413.2783 ],\n",
      "       [ 921.22394,  175.849  , 1082.3665 ,  415.17767]], dtype=float32), array([0.9998055 , 0.99999607], dtype=float32))\n",
      "(array([[ 180.312  ,  168.82443,  366.8345 ,  410.2825 ],\n",
      "       [ 925.98517,  173.35808, 1088.2705 ,  414.66907],\n",
      "       [1077.9264 ,  530.7613 , 1133.928  ,  602.96844]], dtype=float32), array([0.99993634, 0.99999607, 0.91148776], dtype=float32))\n",
      "(array([[ 166.00087,  164.84276,  356.01312,  409.67303],\n",
      "       [ 928.6868 ,  188.8291 , 1085.2284 ,  407.53265]], dtype=float32), array([0.9999169, 0.9999944], dtype=float32))\n",
      "(array([[ 160.93196,  181.52228,  345.23773,  407.15134],\n",
      "       [ 927.83704,  170.18878, 1090.0128 ,  407.93356]], dtype=float32), array([0.99987555, 0.9999938 ], dtype=float32))\n",
      "(array([[ 159.69397,  177.55186,  346.67014,  413.41403],\n",
      "       [ 918.1137 ,  164.65681, 1078.8218 ,  405.7648 ]], dtype=float32), array([0.9998148 , 0.99999857], dtype=float32))\n",
      "(array([[ 152.69247,  179.12921,  336.79697,  406.17136],\n",
      "       [ 913.14404,  162.87384, 1067.5729 ,  399.71994]], dtype=float32), array([0.99993765, 0.99999166], dtype=float32))\n",
      "(array([[ 158.8705 ,  175.6261 ,  343.1927 ,  405.1035 ],\n",
      "       [ 913.4177 ,  155.8966 , 1068.1045 ,  404.39264],\n",
      "       [1062.8384 ,  547.2786 , 1116.036  ,  614.45374]], dtype=float32), array([0.9999349 , 0.99980146, 0.81801   ], dtype=float32))\n",
      "(array([[ 176.84569,  173.056  ,  355.20276,  406.21857],\n",
      "       [ 904.0548 ,  157.90744, 1061.542  ,  406.52475]], dtype=float32), array([0.9997482, 0.9996451], dtype=float32))\n",
      "(array([[ 157.00395,  163.93102,  341.8431 ,  405.29517],\n",
      "       [ 907.376  ,  166.56723, 1075.2229 ,  398.70242],\n",
      "       [1054.6631 ,  448.7499 , 1146.3552 ,  565.80115]], dtype=float32), array([0.9996394, 1.       , 0.816463 ], dtype=float32))\n",
      "(array([[ 158.50706,  170.63394,  337.6408 ,  398.75214],\n",
      "       [ 906.23975,  172.07135, 1071.7065 ,  399.65402],\n",
      "       [1049.8143 ,  443.518  , 1144.3054 ,  565.1928 ]], dtype=float32), array([0.99976844, 1.        , 0.9374919 ], dtype=float32))\n",
      "(array([[ 150.4555 ,  173.79831,  331.4728 ,  402.94214],\n",
      "       [ 895.2356 ,  167.98764, 1053.9523 ,  414.4362 ],\n",
      "       [1040.7571 ,  450.89948, 1130.7109 ,  567.6137 ]], dtype=float32), array([0.99950993, 0.99998915, 0.8915246 ], dtype=float32))\n",
      "(array([[ 149.86356,  175.50763,  328.09924,  403.99124],\n",
      "       [ 876.26105,  177.60477, 1013.763  ,  408.1324 ],\n",
      "       [1026.8502 ,  457.07205, 1114.747  ,  575.6702 ]], dtype=float32), array([0.9994764 , 0.9995409 , 0.90494347], dtype=float32))\n",
      "(array([[134.40921, 183.468  , 308.67883, 403.90305]], dtype=float32), array([0.99956113], dtype=float32))\n",
      "(array([[142.68686, 177.88377, 317.31354, 404.42862]], dtype=float32), array([0.9995809], dtype=float32))\n",
      "(array([[ 730.3358,  148.6059, 1037.4464,  543.33  ]], dtype=float32), array([0.99999964], dtype=float32))\n",
      "(array([[ 740.86694 ,  164.13129 , 1043.2175  ,  541.5787  ],\n",
      "       [   5.369322,  -86.11088 ,  225.02412 ,  213.54823 ]],\n",
      "      dtype=float32), array([0.9999995, 0.8026731], dtype=float32))\n",
      "(array([[ 741.9164 ,  168.14096, 1046.763  ,  536.7273 ]], dtype=float32), array([0.9999993], dtype=float32))\n",
      "(array([[ 747.11414,  157.83987, 1042.0558 ,  532.8802 ]], dtype=float32), array([0.99999774], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "face_detector_mtcnn = MTCNN()\n",
    "for i in range(50):\n",
    "    a = face_detector_mtcnn.detect(batch[i])\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 224.82489,  210.4985 ,  420.9738 ,  463.00748],\n",
       "        [ 835.11536,  201.44376, 1015.41327,  461.78235]], dtype=float32),\n",
       " array([0.9999089 , 0.99991834], dtype=float32))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 58,  28,  19],\n",
       "        [ 58,  28,  19],\n",
       "        [ 58,  28,  19],\n",
       "        ...,\n",
       "        [ 13,  90,   5],\n",
       "        [ 16,  89,   5],\n",
       "        [ 16,  89,   5]],\n",
       "\n",
       "       [[ 58,  28,  19],\n",
       "        [ 58,  28,  19],\n",
       "        [ 56,  27,  18],\n",
       "        ...,\n",
       "        [ 13,  90,   5],\n",
       "        [ 16,  89,   5],\n",
       "        [ 16,  89,   5]],\n",
       "\n",
       "       [[ 59,  30,  22],\n",
       "        [ 59,  30,  22],\n",
       "        [ 59,  30,  22],\n",
       "        ...,\n",
       "        [ 12,  90,   2],\n",
       "        [ 15,  89,   2],\n",
       "        [ 15,  89,   2]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        ...,\n",
       "        [171, 157, 146],\n",
       "        [171, 157, 146],\n",
       "        [171, 157, 146]],\n",
       "\n",
       "       [[251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        ...,\n",
       "        [172, 158, 147],\n",
       "        [172, 158, 147],\n",
       "        [172, 158, 147]],\n",
       "\n",
       "       [[251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        ...,\n",
       "        [172, 158, 147],\n",
       "        [172, 158, 147],\n",
       "        [172, 158, 147]]], dtype=uint8)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.uint8"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 720, 1280, 3)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m face_detector_mtcnn \u001b[39m=\u001b[39m MTCNN()\n\u001b[0;32m----> 2\u001b[0m bboxes, probs \u001b[39m=\u001b[39m face_detector_mtcnn\u001b[39m.\u001b[39;49mdetect(batch)\n\u001b[1;32m      3\u001b[0m faces \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m bbox \u001b[39min\u001b[39;00m bboxes:\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[1;32m    314\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[1;32m    315\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[1;32m    317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    320\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:183\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m    180\u001b[0m     batch_boxes\u001b[39m.\u001b[39mappend(boxes[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m    181\u001b[0m     batch_points\u001b[39m.\u001b[39mappend(points[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m--> 183\u001b[0m batch_boxes, batch_points \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(batch_boxes), np\u001b[39m.\u001b[39marray(batch_points)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m batch_boxes, batch_points\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "face_detector_mtcnn = MTCNN()\n",
    "bboxes, probs = face_detector_mtcnn.detect(batch)\n",
    "faces = []\n",
    "for bbox in bboxes:\n",
    "    xmin, ymin, xmax, ymax = bbox[0]\n",
    "    box = [ymin, ymax, xmin, xmax]\n",
    "    faces.append(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 31, 116,  40],\n",
       "         [ 31, 116,  40],\n",
       "         [ 31, 116,  40],\n",
       "         ...,\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194]],\n",
       "\n",
       "        [[ 31, 116,  40],\n",
       "         [ 31, 116,  40],\n",
       "         [ 31, 116,  40],\n",
       "         ...,\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194]],\n",
       "\n",
       "        [[ 33, 116,  40],\n",
       "         [ 33, 116,  40],\n",
       "         [ 33, 116,  40],\n",
       "         ...,\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[211, 185, 168],\n",
       "         [211, 185, 168],\n",
       "         [210, 183, 167],\n",
       "         ...,\n",
       "         [ 31,   6,  65],\n",
       "         [ 32,   9,  63],\n",
       "         [ 33,  10,  65]],\n",
       "\n",
       "        [[211, 185, 168],\n",
       "         [211, 185, 168],\n",
       "         [210, 183, 167],\n",
       "         ...,\n",
       "         [ 31,   8,  62],\n",
       "         [ 31,   9,  61],\n",
       "         [ 32,  10,  62]],\n",
       "\n",
       "        [[211, 185, 168],\n",
       "         [211, 185, 168],\n",
       "         [210, 183, 167],\n",
       "         ...,\n",
       "         [ 31,   8,  62],\n",
       "         [ 31,   9,  61],\n",
       "         [ 31,   9,  61]]],\n",
       "\n",
       "\n",
       "       [[[ 34, 115,  33],\n",
       "         [ 34, 115,  33],\n",
       "         [ 35, 112,  33],\n",
       "         ...,\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192]],\n",
       "\n",
       "        [[ 34, 115,  33],\n",
       "         [ 33, 113,  32],\n",
       "         [ 35, 112,  33],\n",
       "         ...,\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192]],\n",
       "\n",
       "        [[ 37, 113,  34],\n",
       "         [ 35, 112,  33],\n",
       "         [ 37, 111,  32],\n",
       "         ...,\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[223, 193, 174],\n",
       "         [223, 193, 174],\n",
       "         [223, 193, 174],\n",
       "         ...,\n",
       "         [ 33,   4,  72],\n",
       "         [ 34,   5,  72],\n",
       "         [ 34,   5,  72]],\n",
       "\n",
       "        [[225, 195, 176],\n",
       "         [225, 195, 176],\n",
       "         [225, 195, 176],\n",
       "         ...,\n",
       "         [ 33,   4,  72],\n",
       "         [ 34,   5,  72],\n",
       "         [ 34,   5,  72]],\n",
       "\n",
       "        [[225, 195, 176],\n",
       "         [225, 195, 176],\n",
       "         [225, 195, 176],\n",
       "         ...,\n",
       "         [ 32,   3,  70],\n",
       "         [ 33,   4,  70],\n",
       "         [ 33,   4,  70]]],\n",
       "\n",
       "\n",
       "       [[[ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         ...,\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90]],\n",
       "\n",
       "        [[ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         ...,\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90]],\n",
       "\n",
       "        [[ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         ...,\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         ...,\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102]],\n",
       "\n",
       "        [[229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         ...,\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102]],\n",
       "\n",
       "        [[229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         ...,\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         ...,\n",
       "         [ 49,  87,  15],\n",
       "         [ 49,  87,  15],\n",
       "         [ 49,  87,  15]],\n",
       "\n",
       "        [[ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         ...,\n",
       "         [ 49,  87,  15],\n",
       "         [ 49,  87,  15],\n",
       "         [ 49,  87,  15]],\n",
       "\n",
       "        [[ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         ...,\n",
       "         [ 44,  87,  13],\n",
       "         [ 44,  87,  13],\n",
       "         [ 44,  87,  13]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         ...,\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148]],\n",
       "\n",
       "        [[250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         ...,\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148]],\n",
       "\n",
       "        [[250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         ...,\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148]]],\n",
       "\n",
       "\n",
       "       [[[ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         ...,\n",
       "         [ 25,  87,  10],\n",
       "         [ 25,  87,  10],\n",
       "         [ 25,  87,  10]],\n",
       "\n",
       "        [[ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         ...,\n",
       "         [ 25,  87,  10],\n",
       "         [ 25,  87,  10],\n",
       "         [ 25,  87,  10]],\n",
       "\n",
       "        [[ 59,  30,  22],\n",
       "         [ 59,  30,  22],\n",
       "         [ 59,  30,  22],\n",
       "         ...,\n",
       "         [ 27,  87,   5],\n",
       "         [ 27,  87,   5],\n",
       "         [ 27,  87,   5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148]],\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148]],\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148]]],\n",
       "\n",
       "\n",
       "       [[[ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         ...,\n",
       "         [ 13,  90,   5],\n",
       "         [ 16,  89,   5],\n",
       "         [ 16,  89,   5]],\n",
       "\n",
       "        [[ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         [ 56,  27,  18],\n",
       "         ...,\n",
       "         [ 13,  90,   5],\n",
       "         [ 16,  89,   5],\n",
       "         [ 16,  89,   5]],\n",
       "\n",
       "        [[ 59,  30,  22],\n",
       "         [ 59,  30,  22],\n",
       "         [ 59,  30,  22],\n",
       "         ...,\n",
       "         [ 12,  90,   2],\n",
       "         [ 15,  89,   2],\n",
       "         [ 15,  89,   2]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [171, 157, 146],\n",
       "         [171, 157, 146],\n",
       "         [171, 157, 146]],\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [172, 158, 147],\n",
       "         [172, 158, 147],\n",
       "         [172, 158, 147]],\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [172, 158, 147],\n",
       "         [172, 158, 147],\n",
       "         [172, 158, 147]]]], dtype=uint8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1, 4)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m boxes_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i, bbox \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(boxes):\n\u001b[0;32m----> 3\u001b[0m     xmin, ymin, xmax, ymax \u001b[39m=\u001b[39m bbox\n\u001b[1;32m      4\u001b[0m     box \u001b[39m=\u001b[39m [ymin, ymax, xmin, xmax]\n\u001b[1;32m      5\u001b[0m     boxes_list\u001b[39m.\u001b[39mappend((i, box))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "boxes_list = []\n",
    "for i, bbox in enumerate(boxes):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    box = [ymin, ymax, xmin, xmax]\n",
    "    boxes_list.append((i, box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [62.388275, 314.8143, 534.76434, 730.4663])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "my_test.load_video_from_youtube(yt_link)\n",
    "my_test.total_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty numpy array to hold the frames\n",
    "frames = np.empty((my_test.total_frames, my_test.height, my_test.width, 3), np.dtype('uint8'))\n",
    "\n",
    "my_test.frames_read = 0\n",
    "while True:\n",
    "    ret, frame = self.video.read()\n",
    "    \n",
    "    if not ret:\n",
    "    break\n",
    "\n",
    "# Read the frames in batches and fill up the numpy array\n",
    "for batch_start in range(0, self.frame_count, self.batch_size * self.skip_frames):\n",
    "    batch_end = min(batch_start + (self.batch_size * self.skip_frames), self.frame_count)\n",
    "    batch_index = 0\n",
    "\n",
    "    for i in range(batch_start, batch_end):\n",
    "        ret, frame = self.video.read()\n",
    "        self.frames_read +=1\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if i % self.skip_frames == 0:\n",
    "            frames[batch_index] = frame\n",
    "            batch_index += 1\n",
    "\n",
    "    # Resize the numpy array to fit the actual number of frames in the batch\n",
    "    if batch_index < self.batch_size:\n",
    "        frames = frames[:batch_index]\n",
    "\n",
    "    # Yield the current batch of frames\n",
    "    yield frames\n",
    "\n",
    "# Release the video stream\n",
    "self.video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<1, -1, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDcn = cv::impl::(anonymous namespace)::Set<1, -1, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m face_detector_hcc \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mCascadeClassifier(cv2\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mhaarcascades \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhaarcascade_frontalface_default.xml\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(batch, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[1;32m      3\u001b[0m boxes \u001b[39m=\u001b[39m face_detector_hcc\u001b[39m.\u001b[39mdetectMultiScale(gray, scaleFactor\u001b[39m=\u001b[39m\u001b[39m1.1\u001b[39m, minNeighbors\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m boxes \u001b[39m=\u001b[39m [[y, y\u001b[39m+\u001b[39mh, x, x\u001b[39m+\u001b[39mw] \u001b[39mfor\u001b[39;00m (x,y,w,h) \u001b[39min\u001b[39;00m boxes]\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<1, -1, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDcn = cv::impl::(anonymous namespace)::Set<1, -1, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": [
    "face_detector_hcc = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "gray = cv2.cvtColor(batch, cv2.COLOR_BGR2GRAY)\n",
    "boxes = face_detector_hcc.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "boxes = [[y, y+h, x, x+w] for (x,y,w,h) in boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 50,\n",
       " 'skip_frames': 10,\n",
       " 'yt_link': 'https://www.youtube.com/watch?v=vtT78TfDfXU',\n",
       " 'video': < cv2.VideoCapture 0x14e376470>,\n",
       " 'frame_count': 1785,\n",
       " 'fps': 24,\n",
       " 'height': 720,\n",
       " 'width': 1280,\n",
       " 'total_frames': 179,\n",
       " 'num_batches': 4,\n",
       " 'frames_read': 1785}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_face_det = FaceDetector(detection_type=\"mtcnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'                   # 1 Actor\n",
    "#yt_link = 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "#yt_link = 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "\n",
    "my_test = VideoProcessor(skip_frames=5)\n",
    "my_test.get_video(yt_link)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "writer = cv2.VideoWriter('/Users/ben/neuefische/capstone/NeuralXpresso/notebooks/outputs/Output_video_3.mp4', fourcc, my_test.fps, (my_test.width, my_test.height))\n",
    "counter = 0\n",
    "frames = []\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    for frame in batch:\n",
    "        frames.append(frame)\n",
    "        writer.write(frame)\n",
    "    counter +=1\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'                   # 1 Actor\n",
    "#yt_link = 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "#yt_link = 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "writer = cv2.VideoWriter('/Users/ben/neuefische/capstone/NeuralXpresso/notebooks/outputs/Output_video_3.mp4', fourcc, my_test.fps, (my_test.width, my_test.height))\n",
    "counter = 0\n",
    "my_test.get_video(yt_link)\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    print(f'{idx}: {len(batch)}')\n",
    "    boxes = my_face_detector.detect_faces(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'                   # 1 Actor\n",
    "#yt_link = 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "#yt_link = 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "#my_emotion_detector = EmotionDetector\n",
    "batchlist = []\n",
    "boxlist = []\n",
    "my_test.get_video(yt_link)\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    print(f'{idx}: {len(batch)}')\n",
    "    batchlist.append(batch)\n",
    "    boxes = my_face_detector.detect_faces(batch)\n",
    "    boxlist.append(boxes)\n",
    "    #faces = extract_faces(batch, boxes, my_emotion_detector.input_shape)\n",
    "    #emotions = my_emotion_detector.predict(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_video(video, filename):\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    return cv2.VideoWriter(filename, fourcc, 10, (width,height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_video = YouTube(yt_link)\n",
    "stream = yt_video.streams.get_highest_resolution()   \n",
    "video =  cv2.VideoCapture(stream.url)\n",
    "\n",
    "\n",
    "frame_count = 200\n",
    "batch_size = 52\n",
    "\n",
    "batches = []\n",
    "\n",
    "for batch_start in range(0, frame_count, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, frame_count)\n",
    "    frames = []\n",
    "\n",
    "\n",
    "\n",
    "    # Read the frames in the current batch\n",
    "    for i in range(batch_start, batch_end):\n",
    "        counter = batch_start\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frames.append(frame)\n",
    "        counter+=1\n",
    "\n",
    "        if counter == batch_end:\n",
    "            break\n",
    "    batches.append(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/caffe/caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"deploy.prototxt\" in function 'ReadProtoFromTextFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#     # Load the CNN face detector\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m face_detector \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mdnn\u001b[39m.\u001b[39;49mreadNetFromCaffe(\u001b[39m\"\u001b[39;49m\u001b[39mdeploy.prototxt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mres10_300x300_ssd_iter_140000.caffemodel\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/caffe/caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"deploy.prototxt\" in function 'ReadProtoFromTextFile'\n"
     ]
    }
   ],
   "source": [
    "#     # Load the CNN face detector\n",
    "face_detector = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"res10_300x300_ssd_iter_140000.caffemodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'maha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m maha\n",
      "\u001b[0;31mNameError\u001b[0m: name 'maha' is not defined"
     ]
    }
   ],
   "source": [
    "maha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for norm_box in norm_boxes: \n",
    "        if len(norm_boxes) == 0:\n",
    "            frames_without_faces_counter += 1\n",
    "            continue\n",
    "        \n",
    "        box = np.array([norm_box[0],norm_box[3], norm_box[1], norm_box[2]])\n",
    "        face_landmark = face_recognition.face_landmarks(frame, [box])[0]\n",
    "        if not valid_landmarks(face_landmark):\n",
    "            continue\n",
    "\n",
    "        gray_cropped_face = crop_face(norm_box, frame_pp)\n",
    "\n",
    "        prob = emotions_probability(gray_cropped_face, emotion_classifier)\n",
    "        emotions.append(prob)\n",
    "\n",
    "\n",
    "\n",
    "        max_emotion, max_prob = np.argmax(prob), np.max(prob)\n",
    "        emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "        emotion_text = emotion_labels[max_emotion]\n",
    "\n",
    "\n",
    "\n",
    "        face_encoding = face_recognition.face_encodings(frame, [box])[0]                  \n",
    "        min_distance = float(\"inf\")\n",
    "        matched_individual_id = None\n",
    "\n",
    "        for individual_id, individual_face_encoding in face_embeddings.items():\n",
    "            distance = face_recognition.face_distance([individual_face_encoding], face_encoding)[0]\n",
    "            if distance < 0.60: # Adjust this threshold value based on your requirements\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    matched_individual_id = individual_id\n",
    "\n",
    "        if matched_individual_id is None:\n",
    "            individual_id_counter += 1\n",
    "            matched_individual_id = individual_id_counter\n",
    "            face_embeddings[individual_id_counter] = face_encoding\n",
    "\n",
    "        y_min, y_max, x_min, x_max = norm_box\n",
    "        \n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"Prob: {max_prob:.1%}\", (x_min, y_max + 60), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, f\"{emotion_text}\", (x_min, y_max + 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, f\"ID: {matched_individual_id}\", (x_min, y_min -20), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 1)\n",
    "\n",
    "                \n",
    "        frame_info.append(\n",
    "            (round(video.get(cv2.CAP_PROP_POS_MSEC) / 1000, 2),\n",
    "                current_frame_nr\n",
    "                )\n",
    "        )    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
