{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "from pytube import YouTube\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, batch_size=50, skip_frames=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.skip_frames = skip_frames\n",
    "\n",
    "    def load_video_from_youtube(self, yt_link):\n",
    "        self.yt_link = yt_link\n",
    "        # Download the YouTube video and get the highest resolution stream\n",
    "        yt_video = YouTube(self.yt_link)\n",
    "        stream = yt_video.streams.get_highest_resolution()\n",
    "\n",
    "        # Open the video stream using OpenCV\n",
    "        self.video = cv2.VideoCapture(stream.url)\n",
    "\n",
    "        # Get available and used resolution - Debugging\n",
    "        #self.available_resolutions = [streams.resolution for streams in yt_video.streams.filter(type=\"video\", progressive=True)]\n",
    "        #self.used_resolution = stream.resolution\n",
    "\n",
    "        # Get the number of frames in the video\n",
    "        self.frame_count = int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Get the frame rate of the video\n",
    "        self.fps = int(self.video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "        # Get the height and width of the video frames\n",
    "        self.height = int(self.video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.width = int(self.video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "        # Calculate the total number of frames to process after skipping frames\n",
    "        self.total_frames = len(range(0, self.frame_count, self.skip_frames))\n",
    "\n",
    "        # Calculate the number of batches required to process all the frames\n",
    "        self.num_batches = int(np.ceil(self.total_frames / self.batch_size))\n",
    "\n",
    "    def get_batches(self):\n",
    "        # Initialize an empty numpy array to hold the frames\n",
    "        frames = np.empty((self.batch_size, self.height, self.width, 3), np.dtype('uint8'))\n",
    "\n",
    "        self.frames_read = 0\n",
    "\n",
    "        # Read the frames in batches and fill up the numpy array\n",
    "        for batch_start in range(0, self.frame_count, self.batch_size * self.skip_frames):\n",
    "            batch_end = min(batch_start + (self.batch_size * self.skip_frames), self.frame_count)\n",
    "            batch_index = 0\n",
    "\n",
    "            for i in range(batch_start, batch_end):\n",
    "                ret, frame = self.video.read()\n",
    "                self.frames_read +=1\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                if i % self.skip_frames == 0:\n",
    "                    frames[batch_index] = frame\n",
    "                    batch_index += 1\n",
    "\n",
    "            # Resize the numpy array to fit the actual number of frames in the batch\n",
    "            if batch_index < self.batch_size:\n",
    "                frames = frames[:batch_index]\n",
    "\n",
    "            # Yield the current batch of frames\n",
    "            yield frames\n",
    "\n",
    "        # Release the video stream\n",
    "        self.video.release()\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self, detection_type=\"mtcnn\"):\n",
    "        self.detection_type = detection_type\n",
    "\n",
    "        if self.detection_type == \"mtcnn\":\n",
    "            # Initialize the MTCNN face detector\n",
    "            self.face_detector = MTCNN()\n",
    "        # elif self.detection_type == \"haarcascade\":\n",
    "        #     # Load the Haar Cascade face detector\n",
    "        #     self.face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "        # elif self.detection_type == \"cnn\":\n",
    "        #     # Load the CNN face detector\n",
    "        #     self.face_detector = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "\n",
    "    def detect_faces(self, frames):\n",
    "        # Detect faces in the frames using the selected face detection model\n",
    "        if self.detection_type == \"mtcnn\":\n",
    "            # Use MTCNN face detector\n",
    "            return_boxes, _ = self.face_detector.detect(frames)\n",
    "            return return_boxes\n",
    "            boxes = []\n",
    "            for i, bbox in enumerate(return_boxes[0]):\n",
    "                xmin, ymin, xmax, ymax = bbox\n",
    "                box = [ymin, ymax, xmin, xmax]\n",
    "                boxes.append((i, box))\n",
    "        # elif self.detection_type == \"haarcascade\":\n",
    "        #     # Use Haar Cascade face detector\n",
    "        #     gray = cv2.cvtColor(frames, cv2.COLOR_BGR2GRAY)\n",
    "        #     boxes = self.face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "        #     boxes = [[y, y+h, x, x+w] for (x,y,w,h) in boxes]\n",
    "        # elif self.detection_type == \"cnn\":\n",
    "        #     # Use CNN face detector\n",
    "        #     blob = cv2.dnn.blobFromImages(frames, 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        #     self.face_detector.setInput(blob)\n",
    "        #     detections = self.face_detector.forward()\n",
    "        #     boxes = []\n",
    "        #     for i in range(detections.shape[2]):\n",
    "        #         confidence = detections[0, 0, i, 2]\n",
    "        #         if confidence > 0.5:\n",
    "        #             box = detections[0, 0, i, 3:7] * np.array([frames.shape[2], frames.shape[1], frames.shape[2], frames.shape[1]])\n",
    "        #             box = box.astype(int)\n",
    "        #             ymin, xmin, ymax, xmax = box\n",
    "        #             box = [ymin, ymax, xmin, xmax]\n",
    "        #             boxes.append(box)\n",
    "\n",
    "        return boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 50\n",
      "1: 50\n",
      "2: 50\n",
      "3: 50\n",
      "4: 50\n",
      "5: 50\n",
      "6: 50\n",
      "7: 50\n",
      "8: 50\n",
      "9: 50\n",
      "10: 11\n"
     ]
    }
   ],
   "source": [
    "#yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'                   # 1 Actor\n",
    "#yt_link = 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "#yt_link = 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "yt_link = 'https://www.youtube.com/watch?v=UECCHwh7bZE'\n",
    "\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "#my_emotion_detector = EmotionDetector\n",
    "batchlist = []\n",
    "boxlist = []\n",
    "my_test.load_video_from_youtube(yt_link)\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    print(f'{idx}: {len(batch)}')\n",
    "    batchlist.append(batch)\n",
    "    #boxes = my_face_detector.detect_faces(batch)\n",
    "    #boxlist.append(boxes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m my_test\u001b[39m.\u001b[39mload_video_from_youtube(yt_link)\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(my_test\u001b[39m.\u001b[39mget_batches()):\n\u001b[0;32m----> 7\u001b[0m     boxes \u001b[39m=\u001b[39m my_face_detector\u001b[39m.\u001b[39;49mdetect_faces(batch)\n",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m, in \u001b[0;36mFaceDetector.detect_faces\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect_faces\u001b[39m(\u001b[39mself\u001b[39m, frames):\n\u001b[1;32m     88\u001b[0m     \u001b[39m# Detect faces in the frames using the selected face detection model\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetection_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmtcnn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m         \u001b[39m# Use MTCNN face detector\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m         return_boxes, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mface_detector\u001b[39m.\u001b[39;49mdetect(frames)\n\u001b[1;32m     92\u001b[0m         \u001b[39mreturn\u001b[39;00m return_boxes\n\u001b[1;32m     93\u001b[0m         boxes \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[1;32m    314\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[1;32m    315\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[1;32m    317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    320\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:183\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m    180\u001b[0m     batch_boxes\u001b[39m.\u001b[39mappend(boxes[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m    181\u001b[0m     batch_points\u001b[39m.\u001b[39mappend(points[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m--> 183\u001b[0m batch_boxes, batch_points \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(batch_boxes), np\u001b[39m.\u001b[39marray(batch_points)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m batch_boxes, batch_points\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=wo6K1GWEx84'\n",
    "\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "my_test.load_video_from_youtube(yt_link)\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    boxes = my_face_detector.detect_faces(batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(boxlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(batch)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m batchlist\u001b[39m.\u001b[39mappend(batch)\n\u001b[0;32m---> 11\u001b[0m boxes \u001b[39m=\u001b[39m my_face_detector\u001b[39m.\u001b[39;49mdetect_faces(batch)\n\u001b[1;32m     12\u001b[0m boxlist\u001b[39m.\u001b[39mappend(boxes)\n",
      "Cell \u001b[0;32mIn[64], line 91\u001b[0m, in \u001b[0;36mFaceDetector.detect_faces\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect_faces\u001b[39m(\u001b[39mself\u001b[39m, frames):\n\u001b[1;32m     88\u001b[0m     \u001b[39m# Detect faces in the frames using the selected face detection model\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetection_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmtcnn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m         \u001b[39m# Use MTCNN face detector\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m         return_boxes, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mface_detector\u001b[39m.\u001b[39;49mdetect(frames)\n\u001b[1;32m     92\u001b[0m         \u001b[39mreturn\u001b[39;00m return_boxes\n\u001b[1;32m     93\u001b[0m         boxes \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[1;32m    314\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[1;32m    315\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[1;32m    317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    320\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:183\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m    180\u001b[0m     batch_boxes\u001b[39m.\u001b[39mappend(boxes[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m    181\u001b[0m     batch_points\u001b[39m.\u001b[39mappend(points[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m--> 183\u001b[0m batch_boxes, batch_points \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(batch_boxes), np\u001b[39m.\u001b[39marray(batch_points)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m batch_boxes, batch_points\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=wo6K1GWEx84'\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "#my_emotion_detector = EmotionDetector\n",
    "batchlist = []\n",
    "boxlist = []\n",
    "my_test.load_video_from_youtube(yt_link)\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    print(f'{idx}: {len(batch)}')\n",
    "    batchlist.append(batch)\n",
    "    boxes = my_face_detector.detect_faces(batch)\n",
    "    boxlist.append(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 720, 1280, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[ 800.64   ,  155.62999, 1065.035  ,  477.59427]],\n",
      "\n",
      "       [[ 787.31104,  160.48291, 1047.8341 ,  486.15082]],\n",
      "\n",
      "       [[ 736.2914 ,  159.95483,  996.07245,  482.7968 ]],\n",
      "\n",
      "       [[ 724.2704 ,  159.92754,  975.6582 ,  487.37204]],\n",
      "\n",
      "       [[ 715.5035 ,  159.72684,  956.12256,  473.42117]],\n",
      "\n",
      "       [[ 205.15468,  180.71413,  391.8848 ,  428.30347]],\n",
      "\n",
      "       [[ 213.19006,  178.01108,  399.1574 ,  423.1865 ]],\n",
      "\n",
      "       [[ 227.02106,  182.68604,  408.90018,  420.32648]],\n",
      "\n",
      "       [[ 761.26526,  181.73456, 1000.58075,  492.46167]]], dtype=float32), array([[0.9998952 ],\n",
      "       [0.9997003 ],\n",
      "       [0.99998546],\n",
      "       [0.99999976],\n",
      "       [0.9988213 ],\n",
      "       [0.9996246 ],\n",
      "       [0.99990594],\n",
      "       [0.99990046],\n",
      "       [0.99999857]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "face_detector_mtcnn = MTCNN()\n",
    "a = face_detector_mtcnn.detect(batch[3:12])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1, 4)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ben/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import face_detection\n",
    "# Initialize detector\n",
    "detector = face_detection.build_detector(\"DSFDDetector\", confidence_threshold=.5, nms_iou_threshold=.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting detections\n",
    "detections = detector.detect(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m detections \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39;49mbatched_detect(batch)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/face_detection/base.py:146\u001b[0m, in \u001b[0;36mDetector.batched_detect\u001b[0;34m(self, image, shrink)\u001b[0m\n\u001b[1;32m    144\u001b[0m height, width \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:\u001b[39m3\u001b[39m]\n\u001b[1;32m    145\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_process(image, shrink)\n\u001b[0;32m--> 146\u001b[0m boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batched_detect(image)\n\u001b[1;32m    147\u001b[0m boxes \u001b[39m=\u001b[39m [scale_boxes((height, width), box)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m boxes]\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_detections(boxes)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/face_detection/base.py:126\u001b[0m, in \u001b[0;36mDetector._batched_detect\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batched_detect\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mList[np\u001b[39m.\u001b[39mndarray]:\n\u001b[0;32m--> 126\u001b[0m     boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_detect(image)\n\u001b[1;32m    127\u001b[0m     boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_boxes(boxes)\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_boxes:\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/face_detection/dsfd/detect.py:40\u001b[0m, in \u001b[0;36mDSFDDetector._detect\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m x \u001b[39m=\u001b[39m x[:, [\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m], :, :]\n\u001b[1;32m     39\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_inference):\n\u001b[0;32m---> 40\u001b[0m     boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(\n\u001b[1;32m     41\u001b[0m         x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfidence_threshold, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnms_iou_threshold\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m boxes\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/face_detection/dsfd/face_ssd.py:147\u001b[0m, in \u001b[0;36mSSD.forward\u001b[0;34m(self, x, confidence_threshold, nms_threshold)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m# ResNet152\u001b[39;00m\n\u001b[1;32m    146\u001b[0m conv3_3_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 147\u001b[0m conv4_3_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(conv3_3_x)\n\u001b[1;32m    148\u001b[0m conv5_3_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(conv4_3_x)\n\u001b[1;32m    149\u001b[0m fc7_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(conv5_3_x)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    144\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m--> 146\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    147\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "detections = detector.batched_detect(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting retina-face\n",
      "  Using cached retina_face-0.0.13-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: retina-face\n",
      "Successfully installed retina-face-0.0.13\n"
     ]
    }
   ],
   "source": [
    "!pip install retina-face --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 720, 1280, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/onnx/onnx_importer.cpp:270: error: (-5:Bad argument) Can't read ONNX file: /models/face_detection_yunet_2022mar.onnx in function 'ONNXImporter'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Initialize detector\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m detector \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mFaceDetectorYN\u001b[39m.\u001b[39;49mcreate(\u001b[39m\"\u001b[39;49m\u001b[39m/models/face_detection_yunet_2022mar.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m320\u001b[39;49m, \u001b[39m320\u001b[39;49m))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/onnx/onnx_importer.cpp:270: error: (-5:Bad argument) Can't read ONNX file: /models/face_detection_yunet_2022mar.onnx in function 'ONNXImporter'\n"
     ]
    }
   ],
   "source": [
    "# Initialize detector\n",
    "detector = cv2.FaceDetectorYN.create(\"/models/face_detection_yunet_2022mar.onnx\", \"\", (320, 320))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input size\n",
    "detector.setInputSize((1280, 720))\n",
    "# Getting detections\n",
    "detections = detector.detect(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 213.83145,  219.39542,  415.32016,  483.84225],\n",
      "       [ 834.7785 ,  208.32353, 1019.23663,  466.85956]], dtype=float32), array([0.99976796, 0.99993265], dtype=float32))\n",
      "(array([[ 224.82489,  210.4985 ,  420.9738 ,  463.00748],\n",
      "       [ 835.11536,  201.44376, 1015.41327,  461.78235]], dtype=float32), array([0.9999089 , 0.99991834], dtype=float32))\n",
      "(array([[ 791.4241 ,  163.10724, 1054.3726 ,  494.60812]], dtype=float32), array([0.9998498], dtype=float32))\n",
      "(array([[ 800.64   ,  155.63005, 1065.035  ,  477.5943 ]], dtype=float32), array([0.9998952], dtype=float32))\n",
      "(array([[ 787.31104,  160.48288, 1047.8341 ,  486.1508 ]], dtype=float32), array([0.9997003], dtype=float32))\n",
      "(array([[736.2914 , 159.95485, 996.07245, 482.7968 ]], dtype=float32), array([0.99998546], dtype=float32))\n",
      "(array([[724.2704 , 159.92757, 975.6582 , 487.37204]], dtype=float32), array([0.99999976], dtype=float32))\n",
      "(array([[715.5035 , 159.7268 , 956.12256, 473.4211 ]], dtype=float32), array([0.9988213], dtype=float32))\n",
      "(array([[205.15466, 180.71411, 391.8848 , 428.30344]], dtype=float32), array([0.9996246], dtype=float32))\n",
      "(array([[213.1901 , 178.01111, 399.15738, 423.18646]], dtype=float32), array([0.99990594], dtype=float32))\n",
      "(array([[227.02107, 182.68604, 408.90018, 420.32645]], dtype=float32), array([0.99990046], dtype=float32))\n",
      "(array([[ 761.26526,  181.73456, 1000.5807 ,  492.46158]], dtype=float32), array([0.99999857], dtype=float32))\n",
      "(array([[ 755.8671 ,  181.15637,  985.30756,  477.73108],\n",
      "       [1106.7161 ,  577.9526 , 1169.9537 ,  652.95416]], dtype=float32), array([0.99981326, 0.7708278 ], dtype=float32))\n",
      "(array([[753.9056 , 180.78487, 974.0696 , 482.5361 ]], dtype=float32), array([0.99957734], dtype=float32))\n",
      "(array([[ 754.16943,  208.91489,  968.6512 ,  505.67026],\n",
      "       [1099.8883 ,  589.2897 , 1174.5776 ,  684.39526]], dtype=float32), array([0.9983754 , 0.95600593], dtype=float32))\n",
      "(array([[185.9277 , 211.35156, 373.66745, 447.26688]], dtype=float32), array([0.9992199], dtype=float32))\n",
      "(array([[191.53319 , 207.84862 , 378.68674 , 449.3293  ],\n",
      "       [-25.061834,  87.27451 ,  65.49193 , 202.66249 ]], dtype=float32), array([0.99981695, 0.8056566 ], dtype=float32))\n",
      "(array([[239.01352, 199.12521, 424.81085, 442.0615 ]], dtype=float32), array([0.9998908], dtype=float32))\n",
      "(array([[249.1043 , 190.8    , 427.80307, 427.34714],\n",
      "       [891.7605 , 583.742  , 911.19055, 606.6293 ]], dtype=float32), array([0.99971193, 0.81702673], dtype=float32))\n",
      "(array([[ 793.3452 ,  187.82239, 1025.1099 ,  502.68195]], dtype=float32), array([0.9998803], dtype=float32))\n",
      "(array([[ 806.6299 ,  182.14539, 1037.3386 ,  503.12357]], dtype=float32), array([0.9992404], dtype=float32))\n",
      "(array([[ 807.1734 ,  188.74097, 1036.4078 ,  503.49225]], dtype=float32), array([0.9996092], dtype=float32))\n",
      "(array([[235.81143, 179.23465, 411.17484, 415.74368]], dtype=float32), array([0.99999034], dtype=float32))\n",
      "(array([[ 235.18594,  181.55789,  411.30865,  414.63428],\n",
      "       [1114.435  ,  646.7671 , 1160.2509 ,  702.2205 ]], dtype=float32), array([0.9999881, 0.7103028], dtype=float32))\n",
      "(array([[ 229.61168,  173.42664,  402.60376,  409.19217],\n",
      "       [1111.544  ,  646.5156 , 1159.4585 ,  703.74994]], dtype=float32), array([0.99999166, 0.8855344 ], dtype=float32))\n",
      "(array([[228.8537 , 171.57059, 402.047  , 410.2464 ]], dtype=float32), array([0.9999732], dtype=float32))\n",
      "(array([[ 223.4795 ,  176.69102,  394.9144 ,  411.49283],\n",
      "       [1114.0039 ,  646.9772 , 1160.2795 ,  703.5945 ]], dtype=float32), array([0.99999344, 0.7187914 ], dtype=float32))\n",
      "(array([[218.00972, 174.80247, 391.40262, 414.4093 ]], dtype=float32), array([0.99998116], dtype=float32))\n",
      "(array([[ 751.46967,  153.84312, 1034.2013 ,  520.29047]], dtype=float32), array([0.9998987], dtype=float32))\n",
      "(array([[ 780.4861 ,  157.16771, 1063.3136 ,  523.04974]], dtype=float32), array([0.9999727], dtype=float32))\n",
      "(array([[ 787.6143 ,  163.02722, 1077.2628 ,  518.97736]], dtype=float32), array([0.9999999], dtype=float32))\n",
      "(array([[ 778.96625,  157.48637, 1067.3943 ,  521.8188 ],\n",
      "       [ 162.74947,  182.94966,  358.36838,  434.48328]], dtype=float32), array([0.99999106, 0.9982285 ], dtype=float32))\n",
      "(array([[ 169.899  ,  176.23572,  355.5777 ,  413.2783 ],\n",
      "       [ 921.22394,  175.849  , 1082.3665 ,  415.17767]], dtype=float32), array([0.9998055 , 0.99999607], dtype=float32))\n",
      "(array([[ 180.312  ,  168.82443,  366.8345 ,  410.2825 ],\n",
      "       [ 925.98517,  173.35808, 1088.2705 ,  414.66907],\n",
      "       [1077.9264 ,  530.7613 , 1133.928  ,  602.96844]], dtype=float32), array([0.99993634, 0.99999607, 0.91148776], dtype=float32))\n",
      "(array([[ 166.00087,  164.84276,  356.01312,  409.67303],\n",
      "       [ 928.6868 ,  188.8291 , 1085.2284 ,  407.53265]], dtype=float32), array([0.9999169, 0.9999944], dtype=float32))\n",
      "(array([[ 160.93196,  181.52228,  345.23773,  407.15134],\n",
      "       [ 927.83704,  170.18878, 1090.0128 ,  407.93356]], dtype=float32), array([0.99987555, 0.9999938 ], dtype=float32))\n",
      "(array([[ 159.69397,  177.55186,  346.67014,  413.41403],\n",
      "       [ 918.1137 ,  164.65681, 1078.8218 ,  405.7648 ]], dtype=float32), array([0.9998148 , 0.99999857], dtype=float32))\n",
      "(array([[ 152.69247,  179.12921,  336.79697,  406.17136],\n",
      "       [ 913.14404,  162.87384, 1067.5729 ,  399.71994]], dtype=float32), array([0.99993765, 0.99999166], dtype=float32))\n",
      "(array([[ 158.8705 ,  175.6261 ,  343.1927 ,  405.1035 ],\n",
      "       [ 913.4177 ,  155.8966 , 1068.1045 ,  404.39264],\n",
      "       [1062.8384 ,  547.2786 , 1116.036  ,  614.45374]], dtype=float32), array([0.9999349 , 0.99980146, 0.81801   ], dtype=float32))\n",
      "(array([[ 176.84569,  173.056  ,  355.20276,  406.21857],\n",
      "       [ 904.0548 ,  157.90744, 1061.542  ,  406.52475]], dtype=float32), array([0.9997482, 0.9996451], dtype=float32))\n",
      "(array([[ 157.00395,  163.93102,  341.8431 ,  405.29517],\n",
      "       [ 907.376  ,  166.56723, 1075.2229 ,  398.70242],\n",
      "       [1054.6631 ,  448.7499 , 1146.3552 ,  565.80115]], dtype=float32), array([0.9996394, 1.       , 0.816463 ], dtype=float32))\n",
      "(array([[ 158.50706,  170.63394,  337.6408 ,  398.75214],\n",
      "       [ 906.23975,  172.07135, 1071.7065 ,  399.65402],\n",
      "       [1049.8143 ,  443.518  , 1144.3054 ,  565.1928 ]], dtype=float32), array([0.99976844, 1.        , 0.9374919 ], dtype=float32))\n",
      "(array([[ 150.4555 ,  173.79831,  331.4728 ,  402.94214],\n",
      "       [ 895.2356 ,  167.98764, 1053.9523 ,  414.4362 ],\n",
      "       [1040.7571 ,  450.89948, 1130.7109 ,  567.6137 ]], dtype=float32), array([0.99950993, 0.99998915, 0.8915246 ], dtype=float32))\n",
      "(array([[ 149.86356,  175.50763,  328.09924,  403.99124],\n",
      "       [ 876.26105,  177.60477, 1013.763  ,  408.1324 ],\n",
      "       [1026.8502 ,  457.07205, 1114.747  ,  575.6702 ]], dtype=float32), array([0.9994764 , 0.9995409 , 0.90494347], dtype=float32))\n",
      "(array([[134.40921, 183.468  , 308.67883, 403.90305]], dtype=float32), array([0.99956113], dtype=float32))\n",
      "(array([[142.68686, 177.88377, 317.31354, 404.42862]], dtype=float32), array([0.9995809], dtype=float32))\n",
      "(array([[ 730.3358,  148.6059, 1037.4464,  543.33  ]], dtype=float32), array([0.99999964], dtype=float32))\n",
      "(array([[ 740.86694 ,  164.13129 , 1043.2175  ,  541.5787  ],\n",
      "       [   5.369322,  -86.11088 ,  225.02412 ,  213.54823 ]],\n",
      "      dtype=float32), array([0.9999995, 0.8026731], dtype=float32))\n",
      "(array([[ 741.9164 ,  168.14096, 1046.763  ,  536.7273 ]], dtype=float32), array([0.9999993], dtype=float32))\n",
      "(array([[ 747.11414,  157.83987, 1042.0558 ,  532.8802 ]], dtype=float32), array([0.99999774], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "face_detector_mtcnn = MTCNN()\n",
    "for i in range(50):\n",
    "    a = face_detector_mtcnn.detect(batch[i])\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 224.82489,  210.4985 ,  420.9738 ,  463.00748],\n",
       "        [ 835.11536,  201.44376, 1015.41327,  461.78235]], dtype=float32),\n",
       " array([0.9999089 , 0.99991834], dtype=float32))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 58,  28,  19],\n",
       "        [ 58,  28,  19],\n",
       "        [ 58,  28,  19],\n",
       "        ...,\n",
       "        [ 13,  90,   5],\n",
       "        [ 16,  89,   5],\n",
       "        [ 16,  89,   5]],\n",
       "\n",
       "       [[ 58,  28,  19],\n",
       "        [ 58,  28,  19],\n",
       "        [ 56,  27,  18],\n",
       "        ...,\n",
       "        [ 13,  90,   5],\n",
       "        [ 16,  89,   5],\n",
       "        [ 16,  89,   5]],\n",
       "\n",
       "       [[ 59,  30,  22],\n",
       "        [ 59,  30,  22],\n",
       "        [ 59,  30,  22],\n",
       "        ...,\n",
       "        [ 12,  90,   2],\n",
       "        [ 15,  89,   2],\n",
       "        [ 15,  89,   2]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        ...,\n",
       "        [171, 157, 146],\n",
       "        [171, 157, 146],\n",
       "        [171, 157, 146]],\n",
       "\n",
       "       [[251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        ...,\n",
       "        [172, 158, 147],\n",
       "        [172, 158, 147],\n",
       "        [172, 158, 147]],\n",
       "\n",
       "       [[251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        [251, 221, 202],\n",
       "        ...,\n",
       "        [172, 158, 147],\n",
       "        [172, 158, 147],\n",
       "        [172, 158, 147]]], dtype=uint8)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.uint8"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 720, 1280, 3)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m face_detector_mtcnn \u001b[39m=\u001b[39m MTCNN()\n\u001b[0;32m----> 2\u001b[0m bboxes, probs \u001b[39m=\u001b[39m face_detector_mtcnn\u001b[39m.\u001b[39;49mdetect(batch)\n\u001b[1;32m      3\u001b[0m faces \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m bbox \u001b[39min\u001b[39;00m bboxes:\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[1;32m    314\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[1;32m    315\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[1;32m    317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    320\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/neuefische/capstone/NeuralXpresso/.venv/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:183\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m    180\u001b[0m     batch_boxes\u001b[39m.\u001b[39mappend(boxes[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m    181\u001b[0m     batch_points\u001b[39m.\u001b[39mappend(points[b_i_inds]\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m--> 183\u001b[0m batch_boxes, batch_points \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(batch_boxes), np\u001b[39m.\u001b[39marray(batch_points)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m batch_boxes, batch_points\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "face_detector_mtcnn = MTCNN()\n",
    "bboxes, probs = face_detector_mtcnn.detect(batch)\n",
    "faces = []\n",
    "for bbox in bboxes:\n",
    "    xmin, ymin, xmax, ymax = bbox[0]\n",
    "    box = [ymin, ymax, xmin, xmax]\n",
    "    faces.append(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 31, 116,  40],\n",
       "         [ 31, 116,  40],\n",
       "         [ 31, 116,  40],\n",
       "         ...,\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194]],\n",
       "\n",
       "        [[ 31, 116,  40],\n",
       "         [ 31, 116,  40],\n",
       "         [ 31, 116,  40],\n",
       "         ...,\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194]],\n",
       "\n",
       "        [[ 33, 116,  40],\n",
       "         [ 33, 116,  40],\n",
       "         [ 33, 116,  40],\n",
       "         ...,\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194],\n",
       "         [242, 221, 194]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[211, 185, 168],\n",
       "         [211, 185, 168],\n",
       "         [210, 183, 167],\n",
       "         ...,\n",
       "         [ 31,   6,  65],\n",
       "         [ 32,   9,  63],\n",
       "         [ 33,  10,  65]],\n",
       "\n",
       "        [[211, 185, 168],\n",
       "         [211, 185, 168],\n",
       "         [210, 183, 167],\n",
       "         ...,\n",
       "         [ 31,   8,  62],\n",
       "         [ 31,   9,  61],\n",
       "         [ 32,  10,  62]],\n",
       "\n",
       "        [[211, 185, 168],\n",
       "         [211, 185, 168],\n",
       "         [210, 183, 167],\n",
       "         ...,\n",
       "         [ 31,   8,  62],\n",
       "         [ 31,   9,  61],\n",
       "         [ 31,   9,  61]]],\n",
       "\n",
       "\n",
       "       [[[ 34, 115,  33],\n",
       "         [ 34, 115,  33],\n",
       "         [ 35, 112,  33],\n",
       "         ...,\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192]],\n",
       "\n",
       "        [[ 34, 115,  33],\n",
       "         [ 33, 113,  32],\n",
       "         [ 35, 112,  33],\n",
       "         ...,\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192]],\n",
       "\n",
       "        [[ 37, 113,  34],\n",
       "         [ 35, 112,  33],\n",
       "         [ 37, 111,  32],\n",
       "         ...,\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192],\n",
       "         [243, 224, 192]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[223, 193, 174],\n",
       "         [223, 193, 174],\n",
       "         [223, 193, 174],\n",
       "         ...,\n",
       "         [ 33,   4,  72],\n",
       "         [ 34,   5,  72],\n",
       "         [ 34,   5,  72]],\n",
       "\n",
       "        [[225, 195, 176],\n",
       "         [225, 195, 176],\n",
       "         [225, 195, 176],\n",
       "         ...,\n",
       "         [ 33,   4,  72],\n",
       "         [ 34,   5,  72],\n",
       "         [ 34,   5,  72]],\n",
       "\n",
       "        [[225, 195, 176],\n",
       "         [225, 195, 176],\n",
       "         [225, 195, 176],\n",
       "         ...,\n",
       "         [ 32,   3,  70],\n",
       "         [ 33,   4,  70],\n",
       "         [ 33,   4,  70]]],\n",
       "\n",
       "\n",
       "       [[[ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         ...,\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90]],\n",
       "\n",
       "        [[ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         ...,\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90]],\n",
       "\n",
       "        [[ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         [ 51,  20,  15],\n",
       "         ...,\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90],\n",
       "         [ 58,  66,  90]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         ...,\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102]],\n",
       "\n",
       "        [[229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         ...,\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102]],\n",
       "\n",
       "        [[229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         [229, 190, 171],\n",
       "         ...,\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102],\n",
       "         [ 89,  83, 102]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         ...,\n",
       "         [ 49,  87,  15],\n",
       "         [ 49,  87,  15],\n",
       "         [ 49,  87,  15]],\n",
       "\n",
       "        [[ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         ...,\n",
       "         [ 49,  87,  15],\n",
       "         [ 49,  87,  15],\n",
       "         [ 49,  87,  15]],\n",
       "\n",
       "        [[ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         [ 61,  26,  22],\n",
       "         ...,\n",
       "         [ 44,  87,  13],\n",
       "         [ 44,  87,  13],\n",
       "         [ 44,  87,  13]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         ...,\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148]],\n",
       "\n",
       "        [[250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         ...,\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148]],\n",
       "\n",
       "        [[250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         [250, 222, 203],\n",
       "         ...,\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148],\n",
       "         [171, 159, 148]]],\n",
       "\n",
       "\n",
       "       [[[ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         ...,\n",
       "         [ 25,  87,  10],\n",
       "         [ 25,  87,  10],\n",
       "         [ 25,  87,  10]],\n",
       "\n",
       "        [[ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         ...,\n",
       "         [ 25,  87,  10],\n",
       "         [ 25,  87,  10],\n",
       "         [ 25,  87,  10]],\n",
       "\n",
       "        [[ 59,  30,  22],\n",
       "         [ 59,  30,  22],\n",
       "         [ 59,  30,  22],\n",
       "         ...,\n",
       "         [ 27,  87,   5],\n",
       "         [ 27,  87,   5],\n",
       "         [ 27,  87,   5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148]],\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148]],\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148],\n",
       "         [169, 157, 148]]],\n",
       "\n",
       "\n",
       "       [[[ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         ...,\n",
       "         [ 13,  90,   5],\n",
       "         [ 16,  89,   5],\n",
       "         [ 16,  89,   5]],\n",
       "\n",
       "        [[ 58,  28,  19],\n",
       "         [ 58,  28,  19],\n",
       "         [ 56,  27,  18],\n",
       "         ...,\n",
       "         [ 13,  90,   5],\n",
       "         [ 16,  89,   5],\n",
       "         [ 16,  89,   5]],\n",
       "\n",
       "        [[ 59,  30,  22],\n",
       "         [ 59,  30,  22],\n",
       "         [ 59,  30,  22],\n",
       "         ...,\n",
       "         [ 12,  90,   2],\n",
       "         [ 15,  89,   2],\n",
       "         [ 15,  89,   2]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [171, 157, 146],\n",
       "         [171, 157, 146],\n",
       "         [171, 157, 146]],\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [172, 158, 147],\n",
       "         [172, 158, 147],\n",
       "         [172, 158, 147]],\n",
       "\n",
       "        [[251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         [251, 221, 202],\n",
       "         ...,\n",
       "         [172, 158, 147],\n",
       "         [172, 158, 147],\n",
       "         [172, 158, 147]]]], dtype=uint8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1, 4)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m boxes_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i, bbox \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(boxes):\n\u001b[0;32m----> 3\u001b[0m     xmin, ymin, xmax, ymax \u001b[39m=\u001b[39m bbox\n\u001b[1;32m      4\u001b[0m     box \u001b[39m=\u001b[39m [ymin, ymax, xmin, xmax]\n\u001b[1;32m      5\u001b[0m     boxes_list\u001b[39m.\u001b[39mappend((i, box))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "boxes_list = []\n",
    "for i, bbox in enumerate(boxes):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    box = [ymin, ymax, xmin, xmax]\n",
    "    boxes_list.append((i, box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [62.388275, 314.8143, 534.76434, 730.4663])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "my_test.load_video_from_youtube(yt_link)\n",
    "my_test.total_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty numpy array to hold the frames\n",
    "frames = np.empty((my_test.total_frames, my_test.height, my_test.width, 3), np.dtype('uint8'))\n",
    "\n",
    "my_test.frames_read = 0\n",
    "while True:\n",
    "    ret, frame = self.video.read()\n",
    "    \n",
    "    if not ret:\n",
    "    break\n",
    "\n",
    "# Read the frames in batches and fill up the numpy array\n",
    "for batch_start in range(0, self.frame_count, self.batch_size * self.skip_frames):\n",
    "    batch_end = min(batch_start + (self.batch_size * self.skip_frames), self.frame_count)\n",
    "    batch_index = 0\n",
    "\n",
    "    for i in range(batch_start, batch_end):\n",
    "        ret, frame = self.video.read()\n",
    "        self.frames_read +=1\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if i % self.skip_frames == 0:\n",
    "            frames[batch_index] = frame\n",
    "            batch_index += 1\n",
    "\n",
    "    # Resize the numpy array to fit the actual number of frames in the batch\n",
    "    if batch_index < self.batch_size:\n",
    "        frames = frames[:batch_index]\n",
    "\n",
    "    # Yield the current batch of frames\n",
    "    yield frames\n",
    "\n",
    "# Release the video stream\n",
    "self.video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<1, -1, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDcn = cv::impl::(anonymous namespace)::Set<1, -1, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m face_detector_hcc \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mCascadeClassifier(cv2\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mhaarcascades \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhaarcascade_frontalface_default.xml\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(batch, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[1;32m      3\u001b[0m boxes \u001b[39m=\u001b[39m face_detector_hcc\u001b[39m.\u001b[39mdetectMultiScale(gray, scaleFactor\u001b[39m=\u001b[39m\u001b[39m1.1\u001b[39m, minNeighbors\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m boxes \u001b[39m=\u001b[39m [[y, y\u001b[39m+\u001b[39mh, x, x\u001b[39m+\u001b[39mw] \u001b[39mfor\u001b[39;00m (x,y,w,h) \u001b[39min\u001b[39;00m boxes]\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<1, -1, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDcn = cv::impl::(anonymous namespace)::Set<1, -1, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": [
    "face_detector_hcc = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "gray = cv2.cvtColor(batch, cv2.COLOR_BGR2GRAY)\n",
    "boxes = face_detector_hcc.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "boxes = [[y, y+h, x, x+w] for (x,y,w,h) in boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 50,\n",
       " 'skip_frames': 10,\n",
       " 'yt_link': 'https://www.youtube.com/watch?v=vtT78TfDfXU',\n",
       " 'video': < cv2.VideoCapture 0x14e376470>,\n",
       " 'frame_count': 1785,\n",
       " 'fps': 24,\n",
       " 'height': 720,\n",
       " 'width': 1280,\n",
       " 'total_frames': 179,\n",
       " 'num_batches': 4,\n",
       " 'frames_read': 1785}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_face_det = FaceDetector(detection_type=\"mtcnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'                   # 1 Actor\n",
    "#yt_link = 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "#yt_link = 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "\n",
    "my_test = VideoProcessor(skip_frames=5)\n",
    "my_test.get_video(yt_link)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "writer = cv2.VideoWriter('/Users/ben/neuefische/capstone/NeuralXpresso/notebooks/outputs/Output_video_3.mp4', fourcc, my_test.fps, (my_test.width, my_test.height))\n",
    "counter = 0\n",
    "frames = []\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    for frame in batch:\n",
    "        frames.append(frame)\n",
    "        writer.write(frame)\n",
    "    counter +=1\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'                   # 1 Actor\n",
    "#yt_link = 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "#yt_link = 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "writer = cv2.VideoWriter('/Users/ben/neuefische/capstone/NeuralXpresso/notebooks/outputs/Output_video_3.mp4', fourcc, my_test.fps, (my_test.width, my_test.height))\n",
    "counter = 0\n",
    "my_test.get_video(yt_link)\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    print(f'{idx}: {len(batch)}')\n",
    "    boxes = my_face_detector.detect_faces(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'                   # 1 Actor\n",
    "#yt_link = 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "#yt_link = 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "\n",
    "my_test = VideoProcessor(skip_frames=10)\n",
    "my_face_detector = FaceDetector(detection_type='mtcnn')\n",
    "#my_emotion_detector = EmotionDetector\n",
    "batchlist = []\n",
    "boxlist = []\n",
    "my_test.get_video(yt_link)\n",
    "for idx, batch in enumerate(my_test.get_batches()):\n",
    "    print(f'{idx}: {len(batch)}')\n",
    "    batchlist.append(batch)\n",
    "    boxes = my_face_detector.detect_faces(batch)\n",
    "    boxlist.append(boxes)\n",
    "    #faces = extract_faces(batch, boxes, my_emotion_detector.input_shape)\n",
    "    #emotions = my_emotion_detector.predict(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_video(video, filename):\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    return cv2.VideoWriter(filename, fourcc, 10, (width,height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_video = YouTube(yt_link)\n",
    "stream = yt_video.streams.get_highest_resolution()   \n",
    "video =  cv2.VideoCapture(stream.url)\n",
    "\n",
    "\n",
    "frame_count = 200\n",
    "batch_size = 52\n",
    "\n",
    "batches = []\n",
    "\n",
    "for batch_start in range(0, frame_count, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, frame_count)\n",
    "    frames = []\n",
    "\n",
    "\n",
    "\n",
    "    # Read the frames in the current batch\n",
    "    for i in range(batch_start, batch_end):\n",
    "        counter = batch_start\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frames.append(frame)\n",
    "        counter+=1\n",
    "\n",
    "        if counter == batch_end:\n",
    "            break\n",
    "    batches.append(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/caffe/caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"deploy.prototxt\" in function 'ReadProtoFromTextFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#     # Load the CNN face detector\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m face_detector \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mdnn\u001b[39m.\u001b[39;49mreadNetFromCaffe(\u001b[39m\"\u001b[39;49m\u001b[39mdeploy.prototxt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mres10_300x300_ssd_iter_140000.caffemodel\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/caffe/caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"deploy.prototxt\" in function 'ReadProtoFromTextFile'\n"
     ]
    }
   ],
   "source": [
    "#     # Load the CNN face detector\n",
    "face_detector = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"res10_300x300_ssd_iter_140000.caffemodel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
